{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# register tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine, URL, text\n",
    "\n",
    "from helper.keyword_helper import neo4j_fetch_data, clean_abstracts, make_weight_wordtrie\n",
    "from helper.wordtrie_builder import WordTrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7f4af78e3610>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_LIMIT = 300000\n",
    "NEO4J_CREDENTIALS = {\"url\": \"bolt://localhost:37687\", \"user\": \"neo4j\", \"password\": \"neo4jpassword\"}\n",
    "AHO_LENGTH = 3 # Paper Abstract has to contain AHO_LENGTH or more core keywords to be labeled as ai and deleted from neg sample\n",
    "\n",
    "MAKE_ALL_PLOTS = False\n",
    "\n",
    "url_object = URL.create(\n",
    "    drivername='postgresql+psycopg2', \n",
    "    username='tie',\n",
    "    password='TIE%2023!tuhh',\n",
    "    host='134.28.58.100',\n",
    "    # host='tie-workstation.tail6716.ts.net',\n",
    "    # host='localhost',\n",
    "    port=45432,\n",
    "    database='openalex_db',\n",
    ")\n",
    "engine = create_engine(url_object)\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "Done.\n",
      "Got 330000 papers.\n",
      "\n",
      "Fetching data...\n",
      "Done.\n",
      "Got 255908 OpenAlex IDs from PwC.\n"
     ]
    }
   ],
   "source": [
    "# Load papers from neo4j\n",
    "query = f\"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.abstract IS NOT NULL\n",
    "RETURN p.id AS id, p.abstract AS abstract, p.title AS title\n",
    "ORDER BY rand()\n",
    "LIMIT {SAMPLE_LIMIT + int(SAMPLE_LIMIT * 0.1)}\n",
    "\"\"\"\n",
    "print(\"Fetching data...\")\n",
    "papers_origin = neo4j_fetch_data(query, NEO4J_CREDENTIALS)\n",
    "print(\"Done.\")\n",
    "print(f\"Got {len(papers_origin)} papers.\")\n",
    "print(\"\")\n",
    "\n",
    "query2 = f\"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.id_openalex IS NOT NULL\n",
    "RETURN p.id_openalex AS openalex_id\n",
    "\"\"\"\n",
    "print(\"Fetching data...\")\n",
    "pwc_oalex = neo4j_fetch_data(query2, NEO4J_CREDENTIALS)\n",
    "print(\"Done.\")\n",
    "print(f\"Got {len(pwc_oalex)} OpenAlex IDs from PwC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from Postgres...\n",
      "Got 360000 OpenAlex Works\n"
     ]
    }
   ],
   "source": [
    "sql_query = f'''\n",
    "    SELECT id, title, abstract, abstract_inverted_index\n",
    "    FROM openalex.works TABLESAMPLE BERNOULLI(10)\n",
    "    WHERE abstract_inverted_index IS NOT NULL\n",
    "    LIMIT {SAMPLE_LIMIT + int(SAMPLE_LIMIT * 0.2)}\n",
    "'''\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"Fetching data from Postgres...\")\n",
    "    openalex = pd.read_sql(sql=text(sql_query), con=conn)\n",
    "    print(f\"Got {len(openalex)} OpenAlex Works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from Postgres...\n",
      "Got 18116999 OpenAlex Work IDs with AI\n"
     ]
    }
   ],
   "source": [
    "sql_query3 = f'''\n",
    "    SELECT wc.work_id\n",
    "    FROM openalex.works_concepts wc\n",
    "    JOIN openalex.concepts c ON wc.concept_id = c.id\n",
    "    WHERE c.display_name = 'Artificial intelligence' OR c.display_name = 'Machine learning' OR c.display_name = 'Natural language processing' AND wc.score > 0.2\n",
    "'''\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"Fetching data from Postgres...\")\n",
    "    openalex_ids_with_AI = pd.read_sql(sql=text(sql_query3), con=conn)\n",
    "    print(f\"Got {len(openalex_ids_with_AI)} OpenAlex Work IDs with AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 358823 OpenAlex Works after removing PwC papers\n",
      "Got 323203 OpenAlex Works after removing AI papers\n",
      "Extracting abstract from abstract_inverted_index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8820f709df64c819100b4e0add01746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/323203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language en...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394bf9fa5dff4550be5c13ecf920c474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language en...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9217f42d8bd411b9b6c0a4403da1965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 323203 OpenAlex Works after removing PwC papers\n",
      "Number of rows in OpenAlex Sample after cleaning: 282702 (87.0% remain)\n"
     ]
    }
   ],
   "source": [
    "# Remove all openalex rows where the id is in the pwc_oalex df\n",
    "openalex_id_filtered = openalex[~openalex['id'].isin(pwc_oalex['openalex_id'])]\n",
    "print(f\"Got {len(openalex_id_filtered)} OpenAlex Works after removing PwC papers\")\n",
    "# Remove all openalex rows where the id is in the openalex_ids_with_AI df\n",
    "openalex_id_filtered = openalex_id_filtered[~openalex_id_filtered['id'].isin(openalex_ids_with_AI['work_id'])]\n",
    "print(f\"Got {len(openalex_id_filtered)} OpenAlex Works after removing AI papers\")\n",
    "\n",
    "# Perform cleaning operations\n",
    "# def clean_abstracts(df, inv_index=False, lang='en'):\n",
    "#     if inv_index:\n",
    "#         # Extract and join keys from abstract_inverted_index\n",
    "#         print(\"Extracting abstract from abstract_inverted_index...\")\n",
    "#         df['abstract'] = df['abstract_inverted_index'].progress_apply(lambda x: \" \".join(x['InvertedIndex'].keys()) if x else None)\n",
    "    \n",
    "#     # Regular expressions and string operations\n",
    "#     df['abstract'] = df['abstract'].str.replace(r\"[^a-zA-Z0-9 ]\", \" \", regex=True)\\\n",
    "#                                     .str.replace(r\"abstract\", \"\", flags=re.IGNORECASE)\\\n",
    "#                                     .str.strip()\\\n",
    "#                                     .str.replace(r\"\\s+\", \" \", regex=True)\\\n",
    "#                                     .str.lower()\n",
    "    \n",
    "#     # Filter by abstract length\n",
    "#     df = df[df['abstract'].str.split().str.len() > 10]\n",
    "\n",
    "#     if lang:\n",
    "#         try:\n",
    "#             # Detect language and keep only English abstracts\n",
    "#             import gcld3\n",
    "#             detector = gcld3.NNetLanguageIdentifier(min_num_bytes=0, max_num_bytes=1000)\n",
    "#             print(f\"Detecting language {lang}...\")\n",
    "#             # Sample output \n",
    "#             # print(detector.FindLanguage(text=\"This is a test\").language)\n",
    "#             df['abstract_lang'] = df['abstract'].progress_apply(lambda x: detector.FindLanguage(text=x).language if pd.notna(x) else None)\n",
    "#             df = df[df['abstract_lang'] == lang]\n",
    "#         except:\n",
    "#             print(\"Language detection failed. Keeping all abstracts.\")\n",
    "#             pass\n",
    "\n",
    "#     # Calculate abstract length by counting the words\n",
    "#     df['abstract_length'] = df['abstract'].str.split().str.len()\n",
    "\n",
    "#     return df[df['abstract_length'] > 10]\n",
    "\n",
    "# Clean both DataFrames\n",
    "openalex_sample = clean_abstracts(openalex_id_filtered.copy(), \"abstract\", inv_index=True, lang='en')\n",
    "papers = clean_abstracts(papers_origin.copy(), \"abstract\", inv_index=False, lang='en')\n",
    "\n",
    "# Reset index and apply sample limit if necessary\n",
    "openalex_sample.reset_index(drop=True, inplace=True)\n",
    "if len(openalex_sample) > SAMPLE_LIMIT:\n",
    "    openalex_sample = openalex_sample.sample(n=SAMPLE_LIMIT, random_state=42)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "openalex_sample.drop(columns=['abstract_inverted_index', 'abstract_lang'], inplace=True)\n",
    "papers.drop(columns=['abstract_lang'], inplace=True)\n",
    "\n",
    "# Prints for confirmation\n",
    "print(f\"Got {len(openalex_id_filtered)} OpenAlex Works after removing PwC papers\")\n",
    "print(f\"Number of rows in OpenAlex Sample after cleaning: {len(openalex_sample)} ({round((len(openalex_sample))/len(openalex_id_filtered), 2) * 100}% remain)\")\n",
    "if len(openalex_sample) > SAMPLE_LIMIT:\n",
    "    print(f\"Cut down to {len(openalex_sample)} OpenAlex Works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 315245 rows from dictionary.csv\n"
     ]
    }
   ],
   "source": [
    "# Import the dictionary.csv\n",
    "dict_df = pd.read_csv(\"data/dictionaries/dictionary_kl.csv\")\n",
    "print(f\"Got {len(dict_df)} rows from dictionary.csv\")\n",
    "dict_df[\"trie_id\"] = dict_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315245/315245 [00:04<00:00, 72355.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63414/63414 [00:00<00:00, 101205.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204243/204243 [00:01<00:00, 103172.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47588/47588 [00:00<00:00, 102583.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9390/9390 [00:00<00:00, 105731.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1632/1632 [00:00<00:00, 118259.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5129/5129 [00:00<00:00, 102815.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2629/2629 [00:00<00:00, 112074.90it/s]\n"
     ]
    }
   ],
   "source": [
    "wordtrie = WordTrie(word_filter=True, text_filter=True, show_progress_bar=True, weights=True)\n",
    "\n",
    "trie_dict = {\n",
    "    \"core_all\": wordtrie.from_json(\"data/dictionaries/tries/core_trie.json\"),\n",
    "    \"core_cso\": wordtrie.from_json(\"data/dictionaries/tries/core_cso_trie.json\"),\n",
    "    \"core_method\": wordtrie.from_json(\"data/dictionaries/tries/core_method_trie.json\"),\n",
    "    \"core_task\": wordtrie.from_json(\"data/dictionaries/tries/core_task_trie.json\"),\n",
    "    \"core_dataset\": wordtrie.from_json(\"data/dictionaries/tries/core_dataset_trie.json\"),\n",
    "    \"extended_all\": wordtrie.from_json(\"data/dictionaries/tries/extended_trie.json\"),\n",
    "    \"extended_cso\": wordtrie.from_json(\"data/dictionaries/tries/extended_cso_trie.json\"),\n",
    "    \"extended_method\": wordtrie.from_json(\"data/dictionaries/tries/extended_method_trie.json\"),\n",
    "    \"extended_task\": wordtrie.from_json(\"data/dictionaries/tries/extended_task_trie.json\"),\n",
    "    \"extended_dataset\": wordtrie.from_json(\"data/dictionaries/tries/extended_dataset_trie.json\"),\n",
    "    \"all\": wordtrie.from_json(\"data/dictionaries/tries/all_trie.json\"),\n",
    "    \"cso\": wordtrie.from_json(\"data/dictionaries/tries/all_cso_trie.json\"),\n",
    "    \"method\": wordtrie.from_json(\"data/dictionaries/tries/all_method_trie.json\"),\n",
    "    \"task\": wordtrie.from_json(\"data/dictionaries/tries/all_task_trie.json\"),\n",
    "    \"dataset\": wordtrie.from_json(\"data/dictionaries/tries/all_dataset_trie.json\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_ALL_PLOTS:\n",
    "    # Make a plot for each trie. Remember that it is 3 columns and five rows, e.g. key 0 is in one row with key 5 and key 10\n",
    "    keys = list(trie_dict.keys())\n",
    "    keys_split = [keys[i:i + 2] for i in range(0, len(keys), 2)]\n",
    "    subplot_titles = [f\"{key1} vs {key2}\" for key1, key2 in zip(*keys_split)]\n",
    "    print(subplot_titles)\n",
    "\n",
    "    def process_key(key, df):\n",
    "        wordtrie = trie_dict[key]\n",
    "        print(f\"Processing {key}\")\n",
    "        print(f\"Number of nodes: {wordtrie.count_nodes()}\")\n",
    "        \n",
    "        metadata = df['abstract'].progress_apply(lambda x: wordtrie.aggregate_search_info(x))\n",
    "        df[f\"trie_abs_{key}\"] = metadata.apply(lambda x: x[0])\n",
    "        df[f\"trie_ratio_{key}\"] = metadata.apply(lambda x: x[1])\n",
    "        df[f\"trie_score_{key}\"] = metadata.apply(lambda x: x[2])\n",
    "        df[f\"trie_abs_score_{key}\"] = metadata.apply(lambda x: x[3])\n",
    "\n",
    "    for key in keys:\n",
    "        process_key(key, openalex_sample)\n",
    "        process_key(key, papers)\n",
    "        \n",
    "    combined_figs = [make_subplots(rows=5, cols=3, subplot_titles=subplot_titles) for _ in range(len(keys))]\n",
    "\n",
    "    def add_histograms(figs, df, key, row, col, type, hist_color):\n",
    "        for fig, column in zip(figs, [f'trie_abs_{key}', f'trie_ratio_{key}', f'trie_score_{key}', f'trie_abs_score_{key}']):\n",
    "            fig.add_trace(go.Histogram(x=df[column], opacity=0.75, name=f'{type}: {key}', marker=dict(color=hist_color)), row=row, col=col)\n",
    "            fig.update_yaxes(type='log', row=row, col=col)\n",
    "\n",
    "    for i, (key1, key2) in enumerate(zip(*keys_split)):\n",
    "        add_histograms(combined_figs, openalex_sample, key1, i+1, 1, \"openalex\", \"red\")\n",
    "        add_histograms(combined_figs, papers, key1, i+1, 1, \"papers\", \"blue\")\n",
    "        add_histograms(combined_figs, openalex_sample, key2, i+1, 2, \"openalex\", \"red\")\n",
    "        add_histograms(combined_figs, papers, key2, i+1, 2, \"papers\", \"blue\")\n",
    "\n",
    "    for fig, title in zip(combined_figs, [\"Trie Search Absolute\", \"Trie Search Ratio (len(Words) - len(Abstract)\", \"Trie Search KL Divergence Mean\", \"Trie Search Absolute times KL Divergence Mean\"]):\n",
    "        fig.update_layout(height=2000, width=1800, yaxis_type=\"log\", title_text=f\"Histograms of {title}\")\n",
    "        fig.write_html(f\"plots/trie_histograms_{title.replace(' ', '_').lower()}.html\")\n",
    "\n",
    "    print(\"Saved the figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315245/315245 [00:03<00:00, 96474.77it/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fcafe1013649878b7ef49bfaa79ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/282702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c813ad50784da49d35ac0cef92d9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_df_wo_dataset = dict_df.copy()\n",
    "dict_df_wo_dataset_trie = make_wordtrie(dict_df_wo_dataset[\"keyword\"].tolist(), dict_df_wo_dataset[\"trie_id\"].tolist(), dict_df_wo_dataset[\"kl_divergence_normalized\"].tolist())\n",
    "\n",
    "openalex_metadata = openalex_sample['abstract'].progress_apply(lambda x: dict_df_wo_dataset_trie.aggregate_search_info(x))\n",
    "openalex_sample[f\"test_trie_abs\"] = openalex_metadata.apply(lambda x: x[0])\n",
    "openalex_sample[f\"test_trie_ratio\"] = openalex_metadata.apply(lambda x: x[1])\n",
    "openalex_sample[f\"test_trie_score\"] = openalex_metadata.apply(lambda x: x[2])\n",
    "openalex_sample[f\"test_trie_abs_score\"] = openalex_metadata.apply(lambda x: x[3])\n",
    "\n",
    "papers_metadata = papers['abstract'].progress_apply(lambda x: dict_df_wo_dataset_trie.aggregate_search_info(x))\n",
    "papers[f\"test_trie_abs\"] = papers_metadata.apply(lambda x: x[0])\n",
    "papers[f\"test_trie_ratio\"] = papers_metadata.apply(lambda x: x[1])\n",
    "papers[f\"test_trie_score\"] = papers_metadata.apply(lambda x: x[2])\n",
    "papers[f\"test_trie_abs_score\"] = papers_metadata.apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAlex Negative Sample\n",
      "Mean of test_trie_abs: 20.59784064177574\n",
      "Mean of test_trie_ratio: 0.18722097120086176\n",
      "Mean of test_trie_score: 0.848923369680318\n",
      "Mean of test_trie_abs_score: 17.600327356907243\n",
      "\n",
      "Std of test_trie_abs: 13.212154134605399\n",
      "Std of test_trie_ratio: 0.06695226268371167\n",
      "Std of test_trie_score: 0.04836695337161977\n",
      "Std of test_trie_abs_score: 11.569862026961538\n",
      "\n",
      "PwC AI Papers\n",
      "Mean of test_trie_abs: 50.020033587620176\n",
      "Mean of test_trie_ratio: 0.29103495689170605\n",
      "Mean of test_trie_score: 0.8421660745998888\n",
      "Mean of test_trie_abs_score: 42.21400654403401\n",
      "\n",
      "Std of test_trie_abs: 17.238764537451022\n",
      "Std of test_trie_ratio: 0.06052088305293432\n",
      "Std of test_trie_score: 0.040199481367433475\n",
      "Std of test_trie_abs_score: 14.916351180201836\n"
     ]
    }
   ],
   "source": [
    "openalex_sample_plot = openalex_sample[openalex_sample[\"test_trie_abs_score\"] > 0.01]\n",
    "papers_plot = papers[papers[\"test_trie_abs_score\"] > 0.01]\n",
    "openalex_sample_plot = openalex_sample_plot[openalex_sample_plot[\"test_trie_score\"] < 0.9]\n",
    "papers_plot = papers_plot[papers_plot[\"test_trie_score\"] < 0.9]\n",
    "\n",
    "\n",
    "# Make a subplot with 4 histograms (2x2) \n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=[\"Trie Search Absolute Count\", \"Trie Search Ratio (len(Words) - len(Abstract)\", \"Trie Search KL Divergence Mean\", \"Trie Search Absolute times KL Divergence Mean\"]\n",
    ")\n",
    "\n",
    "# Add histograms\n",
    "fig.add_trace(go.Histogram(x=openalex_sample_plot[\"test_trie_abs\"], opacity=0.75, name=\"negative sample\", marker=dict(color=\"red\")), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=papers_plot[\"test_trie_abs\"], opacity=0.75, name=\"AI Papers\", marker=dict(color=\"blue\")), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=openalex_sample_plot[\"test_trie_ratio\"], opacity=0.75, name=\"negative sample\", marker=dict(color=\"red\")), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=papers_plot[\"test_trie_ratio\"], opacity=0.75, name=\"AI Papers\", marker=dict(color=\"blue\")), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=openalex_sample_plot[\"test_trie_score\"], opacity=0.75, name=\"negative sample\", marker=dict(color=\"red\")), row=2, col=1)\n",
    "fig.add_trace(go.Histogram(x=papers_plot[\"test_trie_score\"], opacity=0.75, name=\"AI Papers\", marker=dict(color=\"blue\")), row=2, col=1)\n",
    "fig.add_trace(go.Histogram(x=openalex_sample_plot[\"test_trie_abs_score\"], opacity=0.75, name=\"negative sample\", marker=dict(color=\"red\")), row=2, col=2)\n",
    "fig.add_trace(go.Histogram(x=papers_plot[\"test_trie_abs_score\"], opacity=0.75, name=\"AI Papers\", marker=dict(color=\"blue\")), row=2, col=2)\n",
    "\n",
    "# Update yaxis to log scale\n",
    "# fig.update_yaxes(type='log', row=1, col=1)\n",
    "# fig.update_yaxes(type='log', row=1, col=2)\n",
    "# fig.update_yaxes(type='log', row=2, col=1)\n",
    "# fig.update_yaxes(type='log', row=2, col=2)\n",
    "\n",
    "# Update title and height\n",
    "fig.update_layout(title_text=\"Histograms of Trie Search Results\")\n",
    "# fig.update_layout(\n",
    "#     annotations=[\n",
    "#         dict(\n",
    "#             x=0.5,  # x-coordinate position of the text box\n",
    "#             y=-0.15,  # y-coordinate position of the text box\n",
    "#             xref=\"paper\",\n",
    "#             yref=\"paper\",\n",
    "#             text=\"Where:<br>D = Dictionary of words<br>T = Analyzed text\",\n",
    "#             showarrow=False\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "fig.write_html(\"plots/test_histograms.html\")\n",
    "\n",
    "# Calculate mean and standard deviation of all 4 columns\n",
    "print(\"OpenAlex Negative Sample\")\n",
    "print(f\"Mean of test_trie_abs: {openalex_sample_plot['test_trie_abs'].mean()}\")\n",
    "print(f\"Mean of test_trie_ratio: {openalex_sample_plot['test_trie_ratio'].mean()}\")\n",
    "print(f\"Mean of test_trie_score: {openalex_sample_plot['test_trie_score'].mean()}\")\n",
    "print(f\"Mean of test_trie_abs_score: {openalex_sample_plot['test_trie_abs_score'].mean()}\")\n",
    "print(\"\")\n",
    "print(f\"Std of test_trie_abs: {openalex_sample_plot['test_trie_abs'].std()}\")\n",
    "print(f\"Std of test_trie_ratio: {openalex_sample_plot['test_trie_ratio'].std()}\")\n",
    "print(f\"Std of test_trie_score: {openalex_sample_plot['test_trie_score'].std()}\")\n",
    "print(f\"Std of test_trie_abs_score: {openalex_sample_plot['test_trie_abs_score'].std()}\")\n",
    "print(\"\")\n",
    "print(\"PwC AI Papers\")\n",
    "print(f\"Mean of test_trie_abs: {papers_plot['test_trie_abs'].mean()}\")\n",
    "print(f\"Mean of test_trie_ratio: {papers_plot['test_trie_ratio'].mean()}\")\n",
    "print(f\"Mean of test_trie_score: {papers_plot['test_trie_score'].mean()}\")\n",
    "print(f\"Mean of test_trie_abs_score: {papers_plot['test_trie_abs_score'].mean()}\")\n",
    "print(\"\")\n",
    "print(f\"Std of test_trie_abs: {papers_plot['test_trie_abs'].std()}\")\n",
    "print(f\"Std of test_trie_ratio: {papers_plot['test_trie_ratio'].std()}\")\n",
    "print(f\"Std of test_trie_score: {papers_plot['test_trie_score'].std()}\")\n",
    "print(f\"Std of test_trie_abs_score: {papers_plot['test_trie_abs_score'].std()}\")\n",
    "\n",
    "# Make boxplots of all 4 columns\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=[\"Trie Search Absolute Count\", \"Trie Search Ratio (len(Words) div len(Abstract)\", \"Trie Search KL Divergence Mean\", \"Trie Search Absolute mul KL Divergence Mean\"],\n",
    "    \n",
    ")\n",
    "\n",
    "# Add boxplots\n",
    "fig.add_trace(go.Box(y=openalex_sample_plot[\"test_trie_abs\"], name=\"negative sample\", marker=dict(color=\"red\")), row=1, col=1)\n",
    "fig.add_trace(go.Box(y=papers_plot[\"test_trie_abs\"], name=\"AI Papers\", marker=dict(color=\"blue\")), row=1, col=1)\n",
    "fig.add_trace(go.Box(y=openalex_sample_plot[\"test_trie_ratio\"], name=\"negative sample\", marker=dict(color=\"red\")), row=1, col=2)\n",
    "fig.add_trace(go.Box(y=papers_plot[\"test_trie_ratio\"], name=\"AI Papers\", marker=dict(color=\"blue\")), row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Box(y=openalex_sample_plot[\"test_trie_score\"], name=\"negative sample\", marker=dict(color=\"red\")), row=2, col=1)\n",
    "fig.add_trace(go.Box(y=papers_plot[\"test_trie_score\"], name=\"AI Papers\", marker=dict(color=\"blue\")), row=2, col=1)\n",
    "fig.add_trace(go.Box(y=openalex_sample_plot[\"test_trie_abs_score\"], name=\"negative sample\", marker=dict(color=\"red\")), row=2, col=2)\n",
    "fig.add_trace(go.Box(y=papers_plot[\"test_trie_abs_score\"], name=\"AI Papers\", marker=dict(color=\"blue\")), row=2, col=2)\n",
    "\n",
    "# Update title and height\n",
    "fig.update_layout(title_text=\"Boxplots of Trie Search Results\")\n",
    "\n",
    "fig.write_html(\"plots/test_boxplots.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
