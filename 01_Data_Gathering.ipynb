{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the latest PapersWithCode Dataset and perform some preprocessing\n",
    "This is the first notebook in the AI Dictionary project. It downloads the latest version of the PapersWithCode dataset and performs some preprocessing. It outputs several JSON files that are then used by the helper script to enrich with generated keywords, embeddings and data from OpenAlex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import hashlib\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Control Variables\n",
    "\n",
    "DOWNLOAD_PWC_DATA = True # If set to true, the PWC data will be downloaded. If files are already there, they will be deleted and downloaded again.\n",
    "\n",
    "PWC_DATA_PATH = \"data/pwc/\" # If this is set to none, the JSON files will be downloaded to the data directory\n",
    "PWC_API_PATH = \"data/pwc_api/\" # If this is set to none, the JSON files will be downloaded to the data directory\n",
    "PWC_PROCESSED_JSON_PATH = \"data/pwc_processed_json/\"\n",
    "NEO4J_PATH = \"data/neo4j/\"\n",
    "NEO4J_DB_NAME = \"pwa1\"\n",
    "\n",
    "if not os.path.exists(PWC_API_PATH):\n",
    "    os.makedirs(PWC_API_PATH)\n",
    "if not os.path.exists(PWC_DATA_PATH):\n",
    "    os.makedirs(PWC_DATA_PATH)\n",
    "if not os.path.exists(NEO4J_PATH):\n",
    "    os.makedirs(NEO4J_PATH)\n",
    "if not os.path.exists(PWC_PROCESSED_JSON_PATH):\n",
    "    os.makedirs(PWC_PROCESSED_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading JSON files from PapersWithCode\n",
      "Deleting old papers.json\n",
      "Deleting old methods.json\n",
      "Deleting old repos.json\n",
      "Deleting old evaluation_tables.json\n",
      "Deleting old datasets.json\n",
      "Downloaded papers.json.gz. Size is 342.64 MB\n",
      "Unzipped papers.json.gz. Size is 1335.52 MB\n",
      "\n",
      "Downloaded methods.json.gz. Size is 0.75 MB\n",
      "Unzipped methods.json.gz. Size is 3.16 MB\n",
      "\n",
      "Downloaded repos.json.gz. Size is 18.33 MB\n",
      "Unzipped repos.json.gz. Size is 114.14 MB\n",
      "\n",
      "Downloaded evaluation_tables.json.gz. Size is 13.69 MB\n",
      "Unzipped evaluation_tables.json.gz. Size is 165.69 MB\n",
      "\n",
      "Downloaded datasets.json.gz. Size is 3.03 MB\n",
      "Unzipped datasets.json.gz. Size is 13.87 MB\n",
      "\n",
      "Downloaded JSON files from PapersWithCode and saved them to data/pwc\n"
     ]
    }
   ],
   "source": [
    "## ONLY EXECUTE IF YOU WANT TO DOWNLOAD THE JSON FILES FROM PAPERSWITHCODE AGAIN!\n",
    "# Download all methods from PapersWithCode at https://production-media.paperswithcode.com/about/methods.json.gz and load it into a dataframe\n",
    "# First load the json from the website into a folder called 'data'. Check if it exists, if not create it.\n",
    "\n",
    "if DOWNLOAD_PWC_DATA:\n",
    "    print(\"Downloading JSON files from PapersWithCode\")\n",
    "    # If there are files in the folder, delete them\n",
    "    if os.path.exists(PWC_DATA_PATH + 'papers.json'):\n",
    "        print(\"Deleting old papers.json\")\n",
    "        os.remove(PWC_DATA_PATH + 'papers.json')\n",
    "    if os.path.exists(PWC_DATA_PATH + 'methods.json'):\n",
    "        print(\"Deleting old methods.json\")\n",
    "        os.remove(PWC_DATA_PATH + 'methods.json')\n",
    "    if os.path.exists(PWC_DATA_PATH + 'repos.json'):\n",
    "        print(\"Deleting old repos.json\")\n",
    "        os.remove(PWC_DATA_PATH + 'repos.json')\n",
    "    if os.path.exists(PWC_DATA_PATH + 'evaluation_tables.json'):\n",
    "        print(\"Deleting old evaluation_tables.json\")\n",
    "        os.remove(PWC_DATA_PATH + 'evaluation_tables.json')\n",
    "    if os.path.exists(PWC_DATA_PATH + 'datasets.json'):\n",
    "        print(\"Deleting old datasets.json\")\n",
    "        os.remove(PWC_DATA_PATH + 'datasets.json')\n",
    "\n",
    "    # Download the json file from the website - Check https://paperswithcode.com/about if the link is still valid\n",
    "    papers_url = 'https://production-media.paperswithcode.com/about/papers-with-abstracts.json.gz'\n",
    "    methods_url = 'https://production-media.paperswithcode.com/about/methods.json.gz'\n",
    "    repos_url = 'https://production-media.paperswithcode.com/about/links-between-papers-and-code.json.gz'\n",
    "    evaluation_tables_url = 'https://production-media.paperswithcode.com/about/evaluation-tables.json.gz'\n",
    "    datasets_url = 'https://production-media.paperswithcode.com/about/datasets.json.gz'\n",
    "\n",
    "    papers_output_path = PWC_DATA_PATH + 'papers.json.gz'\n",
    "    methods_output_path = PWC_DATA_PATH + 'methods.json.gz'\n",
    "    repos_output_path = PWC_DATA_PATH + 'repos.json.gz'\n",
    "    evaluation_tables_output_path = PWC_DATA_PATH + 'evaluation_tables.json.gz'\n",
    "    datasets_output_path = PWC_DATA_PATH + 'datasets.json.gz'\n",
    "\n",
    "    papers_response = requests.get(papers_url)\n",
    "    if papers_response.status_code == 200:\n",
    "        with open(papers_output_path, 'wb') as f:\n",
    "            f.write(papers_response.content)\n",
    "        print(f\"Downloaded papers.json.gz. Size is {round(os.path.getsize(papers_output_path) / 1000000, 2)} MB\")\n",
    "        with gzip.open(PWC_DATA_PATH + 'papers.json.gz', 'rb') as f_in:\n",
    "            with open(PWC_DATA_PATH + 'papers.json', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(PWC_DATA_PATH + 'papers.json.gz')\n",
    "        print(f\"Unzipped papers.json.gz. Size is {round(os.path.getsize(PWC_DATA_PATH + 'papers.json') / 1000000, 2)} MB\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Could not download papers.json.gz\")\n",
    "        print(f\"Status code: {papers_response.status_code}\")\n",
    "        \n",
    "    methods_response = requests.get(methods_url)\n",
    "    if methods_response.status_code == 200:\n",
    "        with open(methods_output_path, 'wb') as f:\n",
    "            f.write(methods_response.content)\n",
    "        print(f\"Downloaded methods.json.gz. Size is {round(os.path.getsize(methods_output_path) / 1000000, 2)} MB\")\n",
    "        with gzip.open(PWC_DATA_PATH + 'methods.json.gz', 'rb') as f_in:\n",
    "            with open(PWC_DATA_PATH + 'methods.json', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(PWC_DATA_PATH + 'methods.json.gz')\n",
    "        print(f\"Unzipped methods.json.gz. Size is {round(os.path.getsize(PWC_DATA_PATH + 'methods.json') / 1000000, 2)} MB\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Could not download methods.json.gz\")\n",
    "        print(f\"Status code: {methods_response.status_code}\")\n",
    "        \n",
    "    repos_response = requests.get(repos_url)\n",
    "    if repos_response.status_code == 200:\n",
    "        with open(repos_output_path, 'wb') as f:\n",
    "            f.write(repos_response.content)\n",
    "        print(f\"Downloaded repos.json.gz. Size is {round(os.path.getsize(repos_output_path) / 1000000, 2)} MB\")\n",
    "        with gzip.open(PWC_DATA_PATH + 'repos.json.gz', 'rb') as f_in:\n",
    "            with open(PWC_DATA_PATH + 'repos.json', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(PWC_DATA_PATH + 'repos.json.gz')\n",
    "        print(f\"Unzipped repos.json.gz. Size is {round(os.path.getsize(PWC_DATA_PATH + 'repos.json') / 1000000, 2)} MB\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Could not download repos.json.gz\")\n",
    "        print(f\"Status code: {repos_response.status_code}\")\n",
    "        \n",
    "    evaluation_tables_response = requests.get(evaluation_tables_url)\n",
    "    if evaluation_tables_response.status_code == 200:\n",
    "        with open(evaluation_tables_output_path, 'wb') as f:\n",
    "            f.write(evaluation_tables_response.content)\n",
    "        print(f\"Downloaded evaluation_tables.json.gz. Size is {round(os.path.getsize(evaluation_tables_output_path) / 1000000, 2)} MB\")\n",
    "        with gzip.open(PWC_DATA_PATH + 'evaluation_tables.json.gz', 'rb') as f_in:\n",
    "            with open(PWC_DATA_PATH + 'evaluation_tables.json', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(PWC_DATA_PATH + 'evaluation_tables.json.gz')\n",
    "        print(f\"Unzipped evaluation_tables.json.gz. Size is {round(os.path.getsize(PWC_DATA_PATH + 'evaluation_tables.json') / 1000000, 2)} MB\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Could not download evaluation_tables.json.gz\")\n",
    "        print(f\"Status code: {evaluation_tables_response.status_code}\")\n",
    "        \n",
    "    datasets_response = requests.get(datasets_url)\n",
    "    if datasets_response.status_code == 200:\n",
    "        with open(datasets_output_path, 'wb') as f:\n",
    "            f.write(datasets_response.content)\n",
    "        print(f\"Downloaded datasets.json.gz. Size is {round(os.path.getsize(datasets_output_path) / 1000000, 2)} MB\")\n",
    "        with gzip.open(PWC_DATA_PATH + 'datasets.json.gz', 'rb') as f_in:\n",
    "            with open(PWC_DATA_PATH + 'datasets.json', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(PWC_DATA_PATH + 'datasets.json.gz')\n",
    "        print(f\"Unzipped datasets.json.gz. Size is {round(os.path.getsize(PWC_DATA_PATH + 'datasets.json') / 1000000, 2)} MB\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Could not download datasets.json.gz\")\n",
    "        print(f\"Status code: {datasets_response.status_code}\")\n",
    "\n",
    "    print(\"Downloaded JSON files from PapersWithCode and saved them to data/pwc\")\n",
    "else:\n",
    "    print(\"Not downloading JSON files from PapersWithCode\")\n",
    "    # Check if the files are already there\n",
    "    if not os.path.exists(PWC_DATA_PATH + 'papers.json'):\n",
    "        print(\"JSON files seem to not exist - set the download variable to True or provide the files here...\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all areas...\n",
      "Done.\n",
      "Getting all tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003b46eb179f4d379a9aebff15b63d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area adversarial. Got 21 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e2808a4f4d44bfb5524e2470486d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area audio. Got 67 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb7b73334724f90add124e204084ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area computer-code. Got 59 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7502ed985f43f890145d68357f61b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area computer-vision. Got 1304 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927c88fe91f94017a7a6dcfafeb05af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area graphs. Got 83 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21724eef9004f4c9343d663901cc44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area knowledge-base. Got 33 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b52c55840634c92bb9cb6198f3cc495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area medical. Got 252 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6af168d77f340f8bbffda1abc0d5d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area methodology. Got 178 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6510a8909a4c7cafc8ca52bab3ec3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area miscellaneous. Got 246 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccf20ef03654ac3a47dabeaeebacf4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area music. Got 24 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85ab197c3784ce0b2a5f9d32e72dc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area natural-language-processing. Got 632 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f28f0f8dcf42a298de0cab8cd40e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area playing-games. Got 42 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b058a555c040d2a0288dc05687a605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area reasoning. Got 63 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1700f7b23a8f4388a431b9805e00f3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area robots. Got 42 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21290cfba97c458db0ac62a4859230be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area speech. Got 77 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860c16bced9c40d5abbd81758dcee806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tasks from area time-series. Got 92 tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027206f916a84b12b2f0015b7ff4c67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved 16 areas with corresponding tasks to data/pwc_api/areas.json.\n"
     ]
    }
   ],
   "source": [
    "if DOWNLOAD_PWC_DATA:\n",
    "    # Get areas and tasks from the PWC API\n",
    "    ITEMS_PER_PAGE = 100\n",
    "\n",
    "    # Get all areas\n",
    "    print(\"Getting all areas...\")\n",
    "    areas = requests.get(\"https://paperswithcode.com/api/v1/areas\").json()\n",
    "    areas = pd.DataFrame(areas[\"results\"])\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Get all tasks from all areas and write them to the datafile\n",
    "    print(\"Getting all tasks...\")\n",
    "    # Make a new column tasks\n",
    "    areas[\"tasks\"] = areas.id.apply(lambda x: [])\n",
    "    page = 1\n",
    "    for area_id in tqdm(areas.id):\n",
    "        # Make a first request to get the count\n",
    "        count = requests.get(f\"https://paperswithcode.com/api/v1/areas/{area_id}/tasks?items_per_page=1\").json()[\"count\"]\n",
    "        print(f\"Getting tasks from area {area_id}. Got {count} tasks...\")\n",
    "        for page in trange(1, int(count / ITEMS_PER_PAGE) + 2):\n",
    "            try:\n",
    "                response = requests.get(f\"https://paperswithcode.com/api/v1/areas/{area_id}/tasks?items_per_page={ITEMS_PER_PAGE}&page={page}\").json()\n",
    "                # Append the results to a list in a new tasks column\n",
    "                if response[\"results\"] is not None:\n",
    "                    areas.loc[areas.id == area_id, \"tasks\"] = areas.loc[areas.id == area_id, \"tasks\"].apply(lambda x: x + response[\"results\"])\n",
    "                else:\n",
    "                    print(f\"Error getting tasks from area {area_id}.\")\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting tasks from area {area_id}.\")\n",
    "                print(e)\n",
    "                print(response)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Write areas to json file\n",
    "    areas.to_json(PWC_API_PATH + \"areas.json\", orient=\"records\")\n",
    "\n",
    "    print(f\"Saved {len(areas)} areas with corresponding tasks to {PWC_API_PATH}areas.json.\")\n",
    "else:\n",
    "    print(\"Not downloading areas and tasks from PapersWithCode\")\n",
    "    # Check if the files are already there\n",
    "    if not os.path.exists(PWC_API_PATH + 'areas.json'):\n",
    "        print(\"JSON files seem to not exist - set the download variable to True or provide the files here...\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON files into dataframes...\n",
      "Loaded JSON files into dataframes\n",
      "\n",
      "Got 410364 papers, 2156 methods, 212222 repos, 8822 datasets and 16 areas.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON files into dataframes\n",
    "print(\"Loading JSON files into dataframes...\")\n",
    "papers_df = pd.read_json(PWC_DATA_PATH + 'papers.json')\n",
    "methods_df = pd.read_json(PWC_DATA_PATH + 'methods.json')\n",
    "repos_df = pd.read_json(PWC_DATA_PATH + 'repos.json')\n",
    "datasets_df = pd.read_json(PWC_DATA_PATH + 'datasets.json')\n",
    "# evaluation_tables_df = pd.read_json(PWC_DATA_PATH + 'evaluation_tables.json')\n",
    "\n",
    "areas_df = pd.read_json(PWC_API_PATH + 'areas.json')\n",
    "print(\"Loaded JSON files into dataframes\")\n",
    "print(\"\")\n",
    "print(f\"Got {len(papers_df)} papers, {len(methods_df)} methods, {len(repos_df)} repos, {len(datasets_df)} datasets and {len(areas_df)} areas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataframes...\n",
      "\n",
      "[NODES] Got 410364 rows in papers_df\n",
      "[NODES] Got 2156 rows in methods_df\n",
      "[NODES] Got 212222 rows in repos_df\n",
      "[NODES] Got 16 rows in areas_df\n",
      "[NODES] Got 3215 rows in tasks_df\n",
      "[NODES] Got 8822 rows in datasets_df\n",
      "[EDGES] Got 15129 rows in datasets_tasks_df\n",
      "[EDGES] Got 669552 rows in papers_tasks_df\n",
      "[EDGES] Got 435782 rows in papers_methods_df\n",
      "[EDGES] Got 212222 rows in papers_repos_df\n",
      "[EDGES] Got 3215 rows in tasks_areas_df\n",
      "\n",
      "Done preprocessing dataframes!\n"
     ]
    }
   ],
   "source": [
    "# Do some preprocessing on the dataframes\n",
    "print(\"Preprocessing dataframes...\")\n",
    "print(\"\")\n",
    "\n",
    "papers_df['id'] = papers_df['paper_url'].apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "methods_df['name'] = methods_df['name'].apply(lambda x: x.lower().replace(\" \", \"-\"))\n",
    "methods_df['id'] = (\"method/\" + methods_df['name']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "repos_df['id'] = repos_df['repo_url'].apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "repos_df['paper_id'] = repos_df['paper_url'].apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "areas_df['id_md5'] = (\"area/\" + areas_df['id']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "print(f\"[NODES] Got {len(papers_df)} rows in papers_df\")\n",
    "print(f\"[NODES] Got {len(methods_df)} rows in methods_df\")\n",
    "print(f\"[NODES] Got {len(repos_df)} rows in repos_df\")\n",
    "print(f\"[NODES] Got {len(areas_df)} rows in areas_df\")\n",
    "\n",
    "# Make a new dataframe tasks_df with all unique tasks taken from the areas_df\n",
    "tasks_df = pd.DataFrame(columns=[\"id\", \"name\", \"description\", \"area_id_md5\"])\n",
    "for index, row in areas_df.iterrows():\n",
    "    for task in row[\"tasks\"]:\n",
    "        tasks_df = pd.concat([tasks_df, pd.DataFrame([[task[\"id\"], task[\"name\"], task[\"description\"], row[\"id_md5\"]]], columns=[\"id\", \"name\", \"description\", \"area_id_md5\"])])\n",
    "tasks_df['id_md5'] = (\"task/\" + tasks_df['id']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "tasks_df = tasks_df.reset_index(drop=True)\n",
    "print(f\"[NODES] Got {len(tasks_df)} rows in tasks_df\")\n",
    "\n",
    "# Make a new dataframe datasets_df and give each dataset a unique id\n",
    "datasets_df['id'] = datasets_df['url'].apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "print(f\"[NODES] Got {len(datasets_df)} rows in datasets_df\")\n",
    "\n",
    "# Create a new dataframe datasets_tasks_df and fill it with all dataset-task pairs from the datasets dataframe Keep in mind that the tasks column contains dicts and they first need to be unpacked and only the field \"task\" has to be taken\n",
    "datasets_tasks_df = pd.DataFrame(datasets_df[['id', 'tasks']].explode('tasks').dropna())\n",
    "datasets_tasks_df['task_id'] = datasets_tasks_df['tasks'].apply(lambda x: x['task'])\n",
    "datasets_tasks_df = datasets_tasks_df.drop(columns=['tasks'])\n",
    "datasets_tasks_df.columns = ['dataset_id', 'task_id']\n",
    "# Make the task_id column a string, lowercase and replace spaces with dashes\n",
    "datasets_tasks_df['task_id'] = datasets_tasks_df['task_id'].apply(lambda x: x.lower().replace(\" \", \"-\"))\n",
    "# Make a new column task_id_md5 in the datasets_tasks_df\n",
    "datasets_tasks_df['task_id_md5'] = (\"task/\" + datasets_tasks_df['task_id']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "# Reset index\n",
    "datasets_tasks_df = datasets_tasks_df.reset_index(drop=True)\n",
    "print(f\"[EDGES] Got {len(datasets_tasks_df)} rows in datasets_tasks_df\")\n",
    "\n",
    "# Create a new dataframe papers_tasks_df and fill it with all paper-task pairs from the papers dataframe.\n",
    "papers_tasks_df = pd.DataFrame(papers_df[['id', 'tasks']].explode('tasks').dropna())\n",
    "papers_tasks_df = papers_tasks_df.reset_index(drop=True)\n",
    "papers_tasks_df.columns = ['paper_id', 'task_id']\n",
    "\n",
    "# Make the task_id column a string, lowercase and replace spaces with dashes\n",
    "papers_tasks_df['task_id'] = papers_tasks_df['task_id'].apply(lambda x: x.lower().replace(\" \", \"-\"))\n",
    "# Make a new column task_id_md5 in the papers_tasks_df\n",
    "papers_tasks_df['task_id_md5'] = (\"task/\" + papers_tasks_df['task_id']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "papers_tasks_df = papers_tasks_df.reset_index(drop=True)\n",
    "print(f\"[EDGES] Got {len(papers_tasks_df)} rows in papers_tasks_df\")\n",
    "\n",
    "papers_methods_df = pd.DataFrame(papers_df[['id', 'methods']].explode('methods').dropna())\n",
    "papers_methods_df.columns = ['paper_id', 'method_id']\n",
    "# Replace the dict in method_id only with tne name value\n",
    "papers_methods_df['method_id'] = papers_methods_df['method_id'].apply(lambda x: x['name'].lower().replace(\" \", \"-\"))\n",
    "# Make a new column method_id_md5 in the papers_methods_df\n",
    "papers_methods_df['method_id_md5'] = (\"method/\" + papers_methods_df['method_id']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "papers_methods_df = papers_methods_df.reset_index(drop=True)\n",
    "print(f\"[EDGES] Got {len(papers_methods_df)} rows in papers_methods_df\")\n",
    "\n",
    "# Make a new dataframe and write only the two columns id and paper_id from the repos_df to it. Then rename the columns \"id\" and \"repo\". Call the df papers_repos_df\n",
    "papers_repos_df = pd.DataFrame(repos_df[['id', 'paper_id']])\n",
    "papers_repos_df.columns = ['repo_id', 'paper_id']\n",
    "papers_repos_df = papers_repos_df.reset_index(drop=True)\n",
    "print(f\"[EDGES] Got {len(papers_repos_df)} rows in papers_repos_df\")\n",
    "\n",
    "tasks_areas_df = pd.DataFrame(tasks_df[['id_md5', 'area_id_md5']])\n",
    "tasks_areas_df.columns = ['id', 'area']\n",
    "tasks_areas_df = tasks_areas_df.reset_index(drop=True)\n",
    "print(f\"[EDGES] Got {len(tasks_areas_df)} rows in tasks_areas_df\"),\n",
    "\n",
    "print(\"\")\n",
    "print(\"Done preprocessing dataframes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing title, abstract and description columns...\n",
      "Done preprocessing title, abstract and description columns!\n"
     ]
    }
   ],
   "source": [
    "# A little preprocessing\n",
    "\n",
    "# Regex remove all HTML tags, *, double spaces, leading and trailing spaces and newlines ffrom the title, abstracts and descriptions of the papers. If None, do nothing\n",
    "print(\"Preprocessing title, abstract and description columns...\")\n",
    "papers_df.title = papers_df.title.apply(lambda x: re.sub('<[^<]+?>|\\\\*|\\n|\\s{2,}', ' ', x) if x is not None else None)\n",
    "papers_df.title = papers_df.title.apply(lambda x: x.strip() if x is not None else None)\n",
    "papers_df.abstract = papers_df.abstract.apply(lambda x: re.sub('<[^<]+?>|\\\\*|\\n|\\s{2,}', ' ', x) if x is not None else None)\n",
    "papers_df.abstract = papers_df.abstract.apply(lambda x: x.strip() if x is not None else None)\n",
    "methods_df.description = methods_df.description.apply(lambda x: re.sub('<[^<]+?>|\\\\*|\\n|\\s{2,}', ' ', x) if x is not None else None)\n",
    "methods_df.description = methods_df.description.apply(lambda x: x.strip() if x is not None else None)\n",
    "datasets_df.description = datasets_df.description.apply(lambda x: re.sub('<[^<]+?>|\\\\*|\\n|\\s{2,}', ' ', x) if x is not None else None)\n",
    "datasets_df.description = datasets_df.description.apply(lambda x: x.strip() if x is not None else None)\n",
    "tasks_df.description = tasks_df.description.apply(lambda x: re.sub('<[^<]+?>|\\\\*|\\n|\\s{2,}', ' ', x) if x is not None else None)\n",
    "tasks_df.description = tasks_df.description.apply(lambda x: x.strip() if x is not None else None)\n",
    "print(\"Done preprocessing title, abstract and description columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataframes to json files...\n",
      "Saved dataframes to json files!\n"
     ]
    }
   ],
   "source": [
    "# Save all dataframes to json files\n",
    "print(\"Saving dataframes to json files...\")\n",
    "papers_df.to_json(PWC_PROCESSED_JSON_PATH + \"papers.json\", orient=\"records\")\n",
    "methods_df.to_json(PWC_PROCESSED_JSON_PATH + \"methods.json\", orient=\"records\")\n",
    "repos_df.to_json(PWC_PROCESSED_JSON_PATH + \"repos.json\", orient=\"records\")\n",
    "datasets_df.to_json(PWC_PROCESSED_JSON_PATH + \"datasets.json\", orient=\"records\")\n",
    "areas_df.to_json(PWC_PROCESSED_JSON_PATH + \"areas.json\", orient=\"records\")\n",
    "tasks_df.to_json(PWC_PROCESSED_JSON_PATH + \"tasks.json\", orient=\"records\")\n",
    "\n",
    "datasets_tasks_df.to_json(PWC_PROCESSED_JSON_PATH + \"datasets_tasks.json\", orient=\"records\")\n",
    "papers_tasks_df.to_json(PWC_PROCESSED_JSON_PATH + \"papers_tasks.json\", orient=\"records\")\n",
    "papers_methods_df.to_json(PWC_PROCESSED_JSON_PATH + \"papers_methods.json\", orient=\"records\")\n",
    "papers_repos_df.to_json(PWC_PROCESSED_JSON_PATH + \"papers_repos.json\", orient=\"records\")\n",
    "tasks_areas_df.to_json(PWC_PROCESSED_JSON_PATH + \"tasks_areas.json\", orient=\"records\")\n",
    "print(\"Saved dataframes to json files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Time\n",
    "Now the time has come to enrich your dataset with other information like embeddings, keywords and OpenAlex Info. This step requires a bit more computing power so it should be done on a GPU. We recommend using the *01a_Data_Gathering_Helper.py* script and running it with tmux. It does the following:\n",
    "\n",
    "- Extract Keywords from Abstracts and Descriptions (using the YAKE framework)\n",
    "- Calculate Embeddings for Titles, Abstracts and Descriptions (using the Sentence Transformers framework and malteos/SciNCL model)\n",
    "- Get OpenAlex Info for each paper (using the OpenAlex Postgres Database)\n",
    "- Get OpenAlex Authors (and edges to papers) for each paper (using the OpenAlex Postgres Database)\n",
    "- Get OpenAlex Institutions (and edges to authors) for each paper (using the OpenAlex Postgres Database)\n",
    "- Get OpenAlex Citiation Info (and edges to papers) for each paper (using the OpenAlex Postgres Database)\n",
    "\n",
    "#### Info:\n",
    "JSONs and CSVs for Authors, Institutions and their respective edges are saved automatically in the filesystem, they do not have to be reimported here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
