{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Extended AI Dictionary\n",
    "This notebook requires a running instance of the neo4j Graph Database with all the data from the steps before loaded and a built core dictionary. It will extend the core dictionary with the data from the graph database and save it as a new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import pickle, os\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "# nlp = spacy.load(\"en_core_web_trf\") # For better accuracy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # For better efficiency\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# register tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "from helper.keyword_helper import process_keywords_for_papers, make_aho_automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 20\n",
    "\n",
    "KEYWORD_FREQ_RANGE = (5,5000)\n",
    "COS_THRESHOLD = 0.95\n",
    "\n",
    "DICT_PATH = \"data/dictionaries\"\n",
    "\n",
    "NEO4J_URL = \"bolt://localhost:37687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"neo4jpassword\"\n",
    "driver = GraphDatabase.driver(NEO4J_URL, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "def fetch_data(query):\n",
    "  with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    return pd.DataFrame([r.values() for r in result], columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "Done.\n",
      "Number of keywords: 3316130\n"
     ]
    }
   ],
   "source": [
    "# Get all keywords related to papers - takes around 2 minutes\n",
    "query = \"\"\"\n",
    "MATCH (p:Paper)-[r]->(k:Keyword)\n",
    "WITH k.keyword AS keyword, p.title AS paper_title, p.id AS paper_id\n",
    "RETURN keyword, paper_title, paper_id\n",
    "\"\"\"\n",
    "print(\"Fetching data...\")\n",
    "paper_keywords = fetch_data(query)\n",
    "print(\"Done.\")\n",
    "print(f\"Number of keywords: {len(paper_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e43eead2614893a72118aac37b4627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3043337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be7aace9b75478fa5f37966c9c1efb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>paper_ids</th>\n",
       "      <th>paper_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reinforcement learning</td>\n",
       "      <td>Learning from Outside the Viability Kernel: Wh...</td>\n",
       "      <td>86954928390154250184510152301458594669</td>\n",
       "      <td>4980</td>\n",
       "      <td>[86954928390154250184510152301458594669, 31546...</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video</td>\n",
       "      <td>Task-Relevant Object Discovery and Categorizat...</td>\n",
       "      <td>212168751528671519369758360408197454323</td>\n",
       "      <td>4800</td>\n",
       "      <td>[212168751528671519369758360408197454323, 1775...</td>\n",
       "      <td>4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Cross-modality image synthesis from unpaired d...</td>\n",
       "      <td>114413421609101646152816570603996432946</td>\n",
       "      <td>4666</td>\n",
       "      <td>[114413421609101646152816570603996432946, 4333...</td>\n",
       "      <td>4666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recent year</td>\n",
       "      <td>Fast Kernelized Correlation Filters without Bo...</td>\n",
       "      <td>149673711123358820267766216923619565226</td>\n",
       "      <td>4554</td>\n",
       "      <td>[149673711123358820267766216923619565226, 9289...</td>\n",
       "      <td>4554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user</td>\n",
       "      <td>Discovering Latent Patterns of Urban Cultural ...</td>\n",
       "      <td>196468073673052363069926166181151783702</td>\n",
       "      <td>4375</td>\n",
       "      <td>[196468073673052363069926166181151783702, 3230...</td>\n",
       "      <td>4375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keyword                                        paper_title  \\\n",
       "0  reinforcement learning  Learning from Outside the Viability Kernel: Wh...   \n",
       "1                   video  Task-Relevant Object Discovery and Categorizat...   \n",
       "2                accuracy  Cross-modality image synthesis from unpaired d...   \n",
       "3             recent year  Fast Kernelized Correlation Filters without Bo...   \n",
       "4                    user  Discovering Latent Patterns of Urban Cultural ...   \n",
       "\n",
       "                                  paper_id  frequency  \\\n",
       "0   86954928390154250184510152301458594669       4980   \n",
       "1  212168751528671519369758360408197454323       4800   \n",
       "2  114413421609101646152816570603996432946       4666   \n",
       "3  149673711123358820267766216923619565226       4554   \n",
       "4  196468073673052363069926166181151783702       4375   \n",
       "\n",
       "                                           paper_ids  paper_counts  \n",
       "0  [86954928390154250184510152301458594669, 31546...          4980  \n",
       "1  [212168751528671519369758360408197454323, 1775...          4800  \n",
       "2  [114413421609101646152816570603996432946, 4333...          4666  \n",
       "3  [149673711123358820267766216923619565226, 9289...          4554  \n",
       "4  [196468073673052363069926166181151783702, 3230...          4375  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dedupe_keywords_f = process_keywords_for_papers(paper_keywords, KEYWORD_FREQ_RANGE)\n",
    "display(dedupe_keywords_f.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model...\n",
      "Done.\n",
      "Embedding all keywords...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66234fa7825e4331b6ab7588a8a29ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading Sentence Transformer model...\")\n",
    "model_scincl = SentenceTransformer('malteos/SciNCL')\n",
    "print(\"Done.\")\n",
    "print(\"Embedding all keywords...\")\n",
    "dedupe_keywords_f['embedding'] = model_scincl.encode(dedupe_keywords_f['keyword'].tolist(), show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the core keywords with their embeddings into a df\n",
    "core_keywords = pd.read_csv('data/dictionaries/core_keywords.csv')\n",
    "core_keywords['embedding'] = core_keywords['embedding'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar keywords...\n",
      "\n",
      "Got 4411 unique extended cso keywords after processing core keywords (10.33%)\n",
      "Got 2167 unique extended method keywords after processing core keywords (5.07%)\n",
      "Got 5769 unique extended task keywords after processing core keywords (13.51%)\n",
      "Got 5763 unique extended dataset keywords after processing core keywords (13.49%)\n",
      "\n",
      "Done saving.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "core_keywords_cso = core_keywords[core_keywords['source'] == 'cso']\n",
    "core_keywords_method = core_keywords[core_keywords['source'] == 'method']\n",
    "core_keywords_task = core_keywords[core_keywords['source'] == 'task']\n",
    "core_keywords_dataset = core_keywords[core_keywords['source'] == 'dataset']\n",
    "\n",
    "# Assuming core_keywords_sample and dedupe_keywords_f are already defined\n",
    "\n",
    "def get_keywords_above_threshold(core_embedding, extended_embeddings, cos_threshold=0.1):\n",
    "    similarities = cosine_similarity([core_embedding], extended_embeddings)[0]\n",
    "    return np.where(similarities > cos_threshold)[0]\n",
    "\n",
    "def batch_process_embeddings(core_embeddings, extended_embeddings, cos_threshold):\n",
    "    with ThreadPoolExecutor(max_workers=NUM_CORES) as executor:\n",
    "        results = list(executor.map(lambda embedding: get_keywords_above_threshold(embedding, extended_embeddings, cos_threshold), core_embeddings))\n",
    "    return results\n",
    "\n",
    "def process_keywords(df, dedupe_keywords, cos_threshold, source):\n",
    "    df['keywords_above_threshold'] = batch_process_embeddings(df['embedding'].tolist(), np.array(dedupe_keywords['embedding'].tolist()), cos_threshold)\n",
    "    df = df.drop(columns=['embedding'])\n",
    "    df['keywords_above_threshold'] = df['keywords_above_threshold'].apply(lambda indices: dedupe_keywords.iloc[indices]['keyword'].tolist())\n",
    "    df = df[['keyword', 'source', 'keywords_above_threshold']]\n",
    "    df['keywords_above_threshold'] = df['keywords_above_threshold'].apply(lambda keywords: list(set(keywords) - set(df['keyword'].tolist())))\n",
    "    df_all = pd.DataFrame({keyword for keywords in df['keywords_above_threshold'] for keyword in keywords}, columns=['keyword'])\n",
    "    df_all['source'] = source\n",
    "    return df_all\n",
    "\n",
    "print(\"Finding similar keywords...\")\n",
    "print(\"\")\n",
    "extended_keywords_cso = process_keywords(core_keywords_cso, dedupe_keywords_f, COS_THRESHOLD, \"cso\")\n",
    "print(f\"Got {len(extended_keywords_cso)} unique extended cso keywords after processing core keywords ({len(extended_keywords_cso) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "extended_keywords_method = process_keywords(core_keywords_method, dedupe_keywords_f, COS_THRESHOLD, \"method\")\n",
    "\n",
    "print(f\"Got {len(extended_keywords_method)} unique extended method keywords after processing core keywords ({len(extended_keywords_method) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "extended_keywords_task = process_keywords(core_keywords_task, dedupe_keywords_f, COS_THRESHOLD, \"task\")\n",
    "\n",
    "print(f\"Got {len(extended_keywords_task)} unique extended task keywords after processing core keywords ({len(extended_keywords_task) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "extended_keywords_dataset = process_keywords(core_keywords_dataset, dedupe_keywords_f, COS_THRESHOLD, \"dataset\")\n",
    "\n",
    "print(f\"Got {len(extended_keywords_dataset)} unique extended dataset keywords after processing core keywords ({len(extended_keywords_dataset) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "print(\"\")\n",
    "extended_keywords = pd.concat([extended_keywords_cso, extended_keywords_method, extended_keywords_task, extended_keywords_dataset])\n",
    "extended_keywords = extended_keywords.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new folder in the DICT_PATH for the ahocorasick dumps\n",
    "if not os.path.exists(DICT_PATH + \"/extended_aho_automation\"):\n",
    "    os.mkdir(DICT_PATH + \"/extended_aho_automation\")\n",
    "    \n",
    "extended_keywords_cso_automation = make_aho_automation(extended_keywords_cso['keyword'].tolist())\n",
    "extended_keywords_cso_automation.save(f\"{DICT_PATH}/extended_aho_automation/cso_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_keywords_method_automation = make_aho_automation(extended_keywords_method['keyword'].tolist())\n",
    "extended_keywords_method_automation.save(f\"{DICT_PATH}/extended_aho_automation/method_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_keywords_task_automation = make_aho_automation(extended_keywords_task['keyword'].tolist())\n",
    "extended_keywords_task_automation.save(f\"{DICT_PATH}/extended_aho_automation/task_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_keywords_dataset_automation = make_aho_automation(extended_keywords_dataset['keyword'].tolist())\n",
    "extended_keywords_dataset_automation.save(f\"{DICT_PATH}/extended_aho_automation/dataset_aho_automation.pkl\", pickle.dumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embeddings for all extended keywords\n",
    "print(\"Embedding all extended keywords...\")\n",
    "extended_keywords['embedding'] = model_scincl.encode(extended_keywords['keyword'].tolist(), show_progress_bar=True).tolist()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the extended keywords to a csv\n",
    "extended_keywords.to_csv('data/dictionaries/extended_keywords.csv', index=False)\n",
    "print(\"Done saving.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
