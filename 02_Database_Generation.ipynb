{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Graph Database from the Data\n",
    "This notebook takes the enriched JSON files returned by the 01a Helper Script and processes them. As a result, several CSV Files are returned that can be used to construct a graph database in neo4j (see bootom of the notebook for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Control Variables\n",
    "PWC_PROCESSED_JSON_PATH = \"data/pwc_processed_json/\"\n",
    "NEO4J_PATH = \"data/neo4j/\"\n",
    "NEO4J_DB_NAME = \"neo4j\"\n",
    "REPLACE_DB = True\n",
    "\n",
    "if not os.path.exists(NEO4J_PATH):\n",
    "    os.makedirs(NEO4J_PATH)\n",
    "if not os.path.exists(PWC_PROCESSED_JSON_PATH):\n",
    "    os.makedirs(PWC_PROCESSED_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating edges between papers and abstract keywords...\n",
      "[EDGES] Got 3045277 rows in papers_abstract_keywords_df\n",
      "Creating edges between papers and title keywords...\n",
      "[EDGES] Got 1284495 rows in papers_title_keywords_df\n",
      "Saving papers_df...\n",
      "Done and deleted papers_df\n"
     ]
    }
   ],
   "source": [
    "# First compute everything for the papers_df, because it is too large to fit in memory with the other dataframes...\n",
    "papers_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'papers_processed.json', dtype={\"id\": str}, encoding=\"utf-8\")\n",
    "paper_keywords = [keyword[0] for row in papers_df[\"abstract_keywords\"] if row is not None for keyword in row if keyword]\n",
    "# Append all keywords from the title_keywords column\n",
    "paper_keywords += [keyword[0] for row in papers_df[\"title_keywords\"] if row is not None for keyword in row if keyword]\n",
    "\n",
    "# Create edges between papers and keywords\n",
    "print(\"Creating edges between papers and abstract keywords...\")\n",
    "papers_abstract_keywords_df = (papers_df[['id', 'abstract_keywords']]\n",
    "                                .rename(columns={'abstract_keywords': 'keywords'})\n",
    "                                .explode('keywords')\n",
    "                                .dropna()\n",
    "                                .assign(keyword=lambda df: df['keywords'].str[0], score=lambda df: df['keywords'].str[1])\n",
    "                                .drop(columns=['keywords'])\n",
    "                                .assign(keyword_id=lambda df: (\"keyword/\" + df['keyword']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16))))\n",
    "                                .drop(columns=['keyword'])\n",
    "                                .rename(columns={'id': 'paper_id'})\n",
    "                                .reindex(columns=['paper_id', 'keyword_id', 'score'])\n",
    "                                .reset_index(drop=True))\n",
    "print(f\"[EDGES] Got {len(papers_abstract_keywords_df)} rows in papers_abstract_keywords_df\")\n",
    "\n",
    "print(\"Creating edges between papers and title keywords...\")\n",
    "papers_title_keywords_df = (papers_df[['id', 'title_keywords']]\n",
    "                                .rename(columns={'title_keywords': 'keywords'})\n",
    "                                .explode('keywords')\n",
    "                                .dropna()\n",
    "                                .assign(keyword=lambda df: df['keywords'].str[0], score=lambda df: df['keywords'].str[1])\n",
    "                                .drop(columns=['keywords'])\n",
    "                                .assign(keyword_id=lambda df: (\"keyword/\" + df['keyword']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16))))\n",
    "                                .drop(columns=['keyword'])\n",
    "                                .rename(columns={'id': 'paper_id'})\n",
    "                                .reindex(columns=['paper_id', 'keyword_id', 'score'])\n",
    "                                .reset_index(drop=True))\n",
    "print(f\"[EDGES] Got {len(papers_title_keywords_df)} rows in papers_title_keywords_df\")\n",
    "\n",
    "# papers_df\n",
    "papers_df = papers_df.rename(columns={\"id\": \"id:ID\"})\n",
    "papers_df = papers_df[[\"id:ID\"] + [col for col in papers_df.columns if col != \"id:ID\"]]\n",
    "papers_df[':LABEL'] = \"Paper\"\n",
    "\n",
    "print(\"Saving papers_df...\")\n",
    "papers_df.to_csv(NEO4J_PATH + \"papers.csv\", index=False)\n",
    "\n",
    "# Delete papers_df to free memory\n",
    "del papers_df\n",
    "print(\"Done and deleted papers_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilinski/.local/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:189: RuntimeWarning: invalid value encountered in cast\n",
      "  return values.astype(dtype, copy=copy)\n"
     ]
    }
   ],
   "source": [
    "# If new embeddings, keywords and OpenAlex Stuff are created, load them into the dataframes\n",
    "# papers_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'papers_processed.json', dtype={\"id\": str}, encoding=\"utf-8\")\n",
    "methods_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'methods_processed.json', dtype={\"id\": str}, encoding=\"utf-8\")\n",
    "tasks_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'tasks_processed.json', dtype={\"area_id_md5\": str, \"id_md5\": str}, encoding=\"utf-8\")\n",
    "areas_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'areas_processed.json', dtype={\"id_md5\": str}, encoding=\"utf-8\")\n",
    "datasets_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'datasets_processed.json', dtype={\"id\": str}, encoding=\"utf-8\")\n",
    "repos_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'repos.json', dtype={\"id\": str}, encoding=\"utf-8\")\n",
    "\n",
    "datasets_tasks_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'datasets_tasks.json', dtype={\"dataset_id\": str, \"task_id\": str, \"task_id_md5\": str}, encoding=\"utf-8\")\n",
    "papers_tasks_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'papers_tasks.json', dtype={\"paper_id\": str, \"task_id\": str, \"task_id_md5\": str}, encoding=\"utf-8\")\n",
    "papers_methods_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'papers_methods.json', dtype={\"paper_id\": str, \"method_id\": str, \"method_id_md5\": str}, encoding=\"utf-8\")\n",
    "papers_repos_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'papers_repos.json', dtype={\"paper_id\": str, \"repo_id\": str}, encoding=\"utf-8\")\n",
    "tasks_areas_df = pd.read_json(PWC_PROCESSED_JSON_PATH + 'tasks_areas.json', dtype={\"id\": str, \"area\": str}, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing all keywords from all dataframes...\n",
      "Got 4395001 keywords from all dataframes!\n",
      "Got 971414 unique keywords from all dataframes (Around 22.10270259324173%).\n",
      "[NODES] Got 971414 rows in keywords_df\n",
      "Creating edges between methods and keywords...\n",
      "[EDGES] Got 11125 rows in methods_keywords_df\n",
      "Creating edges between datasets and keywords...\n",
      "[EDGES] Got 47412 rows in datasets_keywords_df\n",
      "Creating edges between tasks and keywords...\n",
      "[EDGES] Got 6692 rows in tasks_keywords_df\n",
      "Saving keywords and the papers_keywords_df to PWC_PROCESSED_JSON_PATH...\n",
      "Saved keywords and the papers_keywords_df to PWC_PROCESSED_JSON_PATH!\n"
     ]
    }
   ],
   "source": [
    "# Grab all keywords from all dataframes and put them into a list. Remember that the keyword column in each dataframe is a list of dicts with the keyword and the score. I only need the keyword here.\n",
    "print(\"Grabbing all keywords from all dataframes...\")\n",
    "method_keywords = [keyword[0] for row in methods_df[\"description_keywords\"] if row is not None for keyword in row if keyword]\n",
    "dataset_keywords = [keyword[0] for row in datasets_df[\"description_keywords\"] if row is not None for keyword in row if keyword]\n",
    "task_keywords = [keyword[0] for row in tasks_df[\"description_keywords\"] if row is not None for keyword in row if keyword]\n",
    "\n",
    "# Merge the lists and remove duplicates\n",
    "all_keywords = list(paper_keywords + method_keywords + dataset_keywords + task_keywords)\n",
    "print(f\"Got {len(all_keywords)} keywords from all dataframes!\")\n",
    "unique_keywords = list(set(all_keywords))\n",
    "print(f\"Got {len(unique_keywords)} unique keywords from all dataframes (Around {(len(unique_keywords) / len(all_keywords))*100}%).\")\n",
    "\n",
    "# Create a df with two columns: id and keyword. Assign each keyword a unique id the same way as the other nodes\n",
    "keywords_df = pd.DataFrame(unique_keywords, columns=[\"keyword\"])\n",
    "keywords_df['id'] = (\"keyword/\" + keywords_df['keyword']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16)))\n",
    "keywords_df = keywords_df.reset_index(drop=True)\n",
    "# Put the id column in front\n",
    "keywords_df = keywords_df[['id', 'keyword']]\n",
    "print(f\"[NODES] Got {len(keywords_df)} rows in keywords_df\")\n",
    "\n",
    "# Create edges between methods and keywords\n",
    "print(\"Creating edges between methods and keywords...\")\n",
    "methods_keywords_df = (methods_df[['id', 'description_keywords']]\n",
    "                      .rename(columns={'description_keywords': 'keywords'})\n",
    "                      .explode('keywords')\n",
    "                      .dropna()\n",
    "                      .assign(keyword=lambda df: df['keywords'].str[0], score=lambda df: df['keywords'].str[1])\n",
    "                      .drop(columns=['keywords'])\n",
    "                      .assign(keyword_id=lambda df: (\"keyword/\" + df['keyword']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16))))\n",
    "                      .drop(columns=['keyword'])\n",
    "                      .rename(columns={'id': 'method_id'})\n",
    "                      .reindex(columns=['method_id', 'keyword_id', 'score'])\n",
    "                      .reset_index(drop=True))\n",
    "print(f\"[EDGES] Got {len(methods_keywords_df)} rows in methods_keywords_df\")\n",
    "\n",
    "# Create edges between datasets and keywords\n",
    "print(\"Creating edges between datasets and keywords...\")\n",
    "datasets_keywords_df = (datasets_df[['id', 'description_keywords']]\n",
    "                        .rename(columns={'description_keywords': 'keywords'})\n",
    "                      .explode('keywords')\n",
    "                      .dropna()\n",
    "                      .assign(keyword=lambda df: df['keywords'].str[0], score=lambda df: df['keywords'].str[1])\n",
    "                      .drop(columns=['keywords'])\n",
    "                      .assign(keyword_id=lambda df: (\"keyword/\" + df['keyword']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16))))\n",
    "                      .drop(columns=['keyword'])\n",
    "                      .rename(columns={'id': 'dataset_id'})\n",
    "                      .reindex(columns=['dataset_id', 'keyword_id', 'score'])\n",
    "                      .reset_index(drop=True))\n",
    "print(f\"[EDGES] Got {len(datasets_keywords_df)} rows in datasets_keywords_df\")\n",
    "\n",
    "# Create edges between tasks and keywords\n",
    "print(\"Creating edges between tasks and keywords...\")\n",
    "tasks_keywords_df = (tasks_df[['id_md5', 'description_keywords']]\n",
    "                        .rename(columns={'description_keywords': 'keywords'})\n",
    "                      .explode('keywords')\n",
    "                      .dropna()\n",
    "                      .assign(keyword=lambda df: df['keywords'].str[0], score=lambda df: df['keywords'].str[1])\n",
    "                      .drop(columns=['keywords'])\n",
    "                      .assign(keyword_id=lambda df: (\"keyword/\" + df['keyword']).apply(lambda x: str(int(hashlib.md5(x.encode('utf-8')).hexdigest(), 16))))\n",
    "                      .drop(columns=['keyword'])\n",
    "                      .rename(columns={'id_md5': 'task_id'})\n",
    "                      .reindex(columns=['task_id', 'keyword_id', 'score'])\n",
    "                      .reset_index(drop=True))\n",
    "print(f\"[EDGES] Got {len(tasks_keywords_df)} rows in tasks_keywords_df\")\n",
    "\n",
    "# Save the keywords and the papers_keywords_df to PWC_PROCESSED_JSON_PATH\n",
    "print(\"Saving keywords and the papers_keywords_df to PWC_PROCESSED_JSON_PATH...\")\n",
    "keywords_df.to_json(PWC_PROCESSED_JSON_PATH + \"keywords.json\", orient=\"records\")\n",
    "papers_title_keywords_df.to_json(PWC_PROCESSED_JSON_PATH + \"papers_title_keywords_df.json\", orient=\"records\")\n",
    "papers_abstract_keywords_df.to_json(PWC_PROCESSED_JSON_PATH + \"papers_abstract_keywords_df.json\", orient=\"records\")\n",
    "methods_keywords_df.to_json(PWC_PROCESSED_JSON_PATH + \"methods_keywords.json\", orient=\"records\")\n",
    "datasets_keywords_df.to_json(PWC_PROCESSED_JSON_PATH + \"datasets_keywords.json\", orient=\"records\")\n",
    "tasks_keywords_df.to_json(PWC_PROCESSED_JSON_PATH + \"tasks_keywords.json\", orient=\"records\")\n",
    "print(\"Saved keywords and the papers_keywords_df to PWC_PROCESSED_JSON_PATH!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for each keyword and store them as features in the keywords_df...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/wilinski/.cache/torch/sentence_transformers/malteos_SciNCL. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60416924bd504bf5a874e19d8575fdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating embeddings for each keyword and store them as features in the keywords_df!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Calculate embeddings for each keyword and store them as features in the keywords_df\n",
    "print(\"Calculating embeddings for each keyword and store them as features in the keywords_df...\")\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('malteos/SciNCL')\n",
    "# Calculate the embeddings for each keyword\n",
    "keyword_embeddings = model.encode(keywords_df.keyword.tolist(), show_progress_bar=True)\n",
    "# Add the embeddings to the keywords_df\n",
    "keywords_df[\"embedding\"] = keyword_embeddings.tolist()\n",
    "print(\"Done calculating embeddings for each keyword and store them as features in the keywords_df!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CSV importable files for neo4j...\n",
      "Done creating CSV importable files for neo4j!\n",
      "Saving CSV files to neo4j import folder...\n",
      "Done saving CSV files to neo4j import folder!\n"
     ]
    }
   ],
   "source": [
    "# Create CSV importable files form neo4j\n",
    "print(\"Creating CSV importable files for neo4j...\")\n",
    "\n",
    "# Change the column names slightly to match the neo4j import format https://neo4j.com/docs/operations-manual/current/tutorial/neo4j-admin-import/\n",
    "# # papers_df\n",
    "# papers_df = papers_df.rename(columns={\"id\": \"id:ID\"})\n",
    "# papers_df = papers_df[[\"id:ID\"] + [col for col in papers_df.columns if col != \"id:ID\"]]\n",
    "# papers_df[':LABEL'] = \"Paper\"\n",
    "# methods_df\n",
    "methods_df = methods_df.rename(columns={\"id\": \"id:ID\"})\n",
    "methods_df['id:ID'] = methods_df['id:ID'].astype(str)\n",
    "methods_df = methods_df[[\"id:ID\"] + [col for col in methods_df.columns if col != \"id:ID\"]]\n",
    "methods_df[':LABEL'] = \"Method\"\n",
    "# repos_df\n",
    "repos_df = repos_df.rename(columns={\"id\": \"id:ID\"})\n",
    "repos_df['id:ID'] = repos_df['id:ID'].astype(str)\n",
    "repos_df = repos_df[[\"id:ID\"] + [col for col in repos_df.columns if col != \"id:ID\"]]\n",
    "repos_df[':LABEL'] = \"Repo\"\n",
    "# areas_df\n",
    "areas_df = areas_df.rename(columns={\"id_md5\": \"id:ID\"})\n",
    "areas_df['id:ID'] = areas_df['id:ID'].astype(str)\n",
    "areas_df = areas_df[[\"id:ID\"] + [col for col in areas_df.columns if col != \"id:ID\"]]\n",
    "areas_df[':LABEL'] = \"Area\"\n",
    "# tasks_df\n",
    "tasks_df = tasks_df.rename(columns={\"id_md5\": \"id:ID\"})\n",
    "tasks_df['id:ID'] = tasks_df['id:ID'].astype(str)\n",
    "tasks_df = tasks_df[[\"id:ID\"] + [col for col in tasks_df.columns if col != \"id:ID\"]]\n",
    "tasks_df[':LABEL'] = \"Task\"\n",
    "# datasets_df\n",
    "datasets_df = datasets_df.rename(columns={\"id\": \"id:ID\"})\n",
    "datasets_df['id:ID'] = datasets_df['id:ID'].astype(str)\n",
    "datasets_df = datasets_df[[\"id:ID\"] + [col for col in datasets_df.columns if col != \"id:ID\"]]\n",
    "datasets_df[':LABEL'] = \"Dataset\"\n",
    "# keywords_df\n",
    "keywords_df = keywords_df.rename(columns={\"id\": \"id:ID\"})\n",
    "keywords_df['id:ID'] = keywords_df['id:ID'].astype(str)\n",
    "keywords_df = keywords_df[[\"id:ID\"] + [col for col in keywords_df.columns if col != \"id:ID\"]]\n",
    "keywords_df[':LABEL'] = \"Keyword\"\n",
    "\n",
    "# datasets_tasks_df\n",
    "datasets_tasks_df = datasets_tasks_df.rename(columns={\"dataset_id\": \":START_ID\", \"task_id_md5\": \":END_ID\"})\n",
    "datasets_tasks_df[':START_ID'] = datasets_tasks_df[':START_ID'].astype(str)\n",
    "datasets_tasks_df[':END_ID'] = datasets_tasks_df[':END_ID'].astype(str)\n",
    "# datasets_tasks_df = datasets_tasks_df.drop(columns=['dataset_id'])\n",
    "datasets_tasks_df[':TYPE'] = \"HAS_TASK\"\n",
    "# papers_tasks_df\n",
    "papers_tasks_df = papers_tasks_df.rename(columns={\"paper_id\": \":START_ID\", \"task_id_md5\": \":END_ID\"})\n",
    "papers_tasks_df[':START_ID'] = papers_tasks_df[':START_ID'].astype(str)\n",
    "papers_tasks_df[':END_ID'] = papers_tasks_df[':END_ID'].astype(str)\n",
    "# papers_tasks_df = papers_tasks_df.drop(columns=['task_id'])\n",
    "papers_tasks_df[':TYPE'] = \"HAS_TASK\"\n",
    "# papers_methods_df\n",
    "papers_methods_df = papers_methods_df.rename(columns={\"paper_id\": \":START_ID\", \"method_id_md5\": \":END_ID\"})\n",
    "papers_methods_df[':START_ID'] = papers_methods_df[':START_ID'].astype(str)\n",
    "papers_methods_df[':END_ID'] = papers_methods_df[':END_ID'].astype(str)\n",
    "# papers_methods_df = papers_methods_df.drop(columns=['method_id'])\n",
    "papers_methods_df[':TYPE'] = \"HAS_METHOD\"\n",
    "# papers_repos_df\n",
    "papers_repos_df = papers_repos_df.rename(columns={\"paper_id\": \":START_ID\", \"repo_id\": \":END_ID\"})\n",
    "papers_repos_df[':START_ID'] = papers_repos_df[':START_ID'].astype(str)\n",
    "papers_repos_df[':END_ID'] = papers_repos_df[':END_ID'].astype(str)\n",
    "papers_repos_df[':TYPE'] = \"HAS_REPO\"\n",
    "# tasks_areas_df\n",
    "tasks_areas_df = tasks_areas_df.rename(columns={\"id\": \":START_ID\", \"area\": \":END_ID\"})\n",
    "tasks_areas_df[':START_ID'] = tasks_areas_df[':START_ID'].astype(str)\n",
    "tasks_areas_df[':END_ID'] = tasks_areas_df[':END_ID'].astype(str)\n",
    "tasks_areas_df[':TYPE'] = \"HAS_AREA\"\n",
    "# papers_title_keywords_df\n",
    "papers_title_keywords_df = papers_title_keywords_df.rename(columns={\"paper_id\": \":START_ID\", \"keyword_id\": \":END_ID\"})\n",
    "papers_title_keywords_df[':START_ID'] = papers_title_keywords_df[':START_ID'].astype(str)\n",
    "papers_title_keywords_df[':END_ID'] = papers_title_keywords_df[':END_ID'].astype(str)\n",
    "papers_title_keywords_df['source'] = \"title\"\n",
    "papers_title_keywords_df[':TYPE'] = \"HAS_KEYWORD\"\n",
    "# papers_abstract_keywords_df\n",
    "papers_abstract_keywords_df = papers_abstract_keywords_df.rename(columns={\"paper_id\": \":START_ID\", \"keyword_id\": \":END_ID\"})\n",
    "papers_abstract_keywords_df[':START_ID'] = papers_abstract_keywords_df[':START_ID'].astype(str)\n",
    "papers_abstract_keywords_df[':END_ID'] = papers_abstract_keywords_df[':END_ID'].astype(str)\n",
    "papers_abstract_keywords_df['source'] = \"abstract\"\n",
    "papers_abstract_keywords_df[':TYPE'] = \"HAS_KEYWORD\"\n",
    "# methods_keywords_df\n",
    "methods_keywords_df = methods_keywords_df.rename(columns={\"method_id\": \":START_ID\", \"keyword_id\": \":END_ID\"})\n",
    "methods_keywords_df[':START_ID'] = methods_keywords_df[':START_ID'].astype(str)\n",
    "methods_keywords_df[':END_ID'] = methods_keywords_df[':END_ID'].astype(str)\n",
    "methods_keywords_df['source'] = \"description\"\n",
    "methods_keywords_df[':TYPE'] = \"HAS_KEYWORD\"\n",
    "# datasets_keywords_df\n",
    "datasets_keywords_df = datasets_keywords_df.rename(columns={\"dataset_id\": \":START_ID\", \"keyword_id\": \":END_ID\"})\n",
    "datasets_keywords_df[':START_ID'] = datasets_keywords_df[':START_ID'].astype(str)\n",
    "datasets_keywords_df[':END_ID'] = datasets_keywords_df[':END_ID'].astype(str)\n",
    "datasets_keywords_df['source'] = \"description\"\n",
    "datasets_keywords_df[':TYPE'] = \"HAS_KEYWORD\"\n",
    "# tasks_keywords_df\n",
    "tasks_keywords_df = tasks_keywords_df.rename(columns={\"task_id\": \":START_ID\", \"keyword_id\": \":END_ID\"})\n",
    "tasks_keywords_df[':START_ID'] = tasks_keywords_df[':START_ID'].astype(str)\n",
    "tasks_keywords_df[':END_ID'] = tasks_keywords_df[':END_ID'].astype(str)\n",
    "tasks_keywords_df['source'] = \"description\"\n",
    "tasks_keywords_df[':TYPE'] = \"HAS_KEYWORD\"\n",
    "print(\"Done creating CSV importable files for neo4j!\")\n",
    "\n",
    "# Save the dataframes to csv files into the neo4j import folder\n",
    "print(\"Saving CSV files to neo4j import folder...\")\n",
    "# papers_df.to_csv(NEO4J_PATH + \"papers.csv\", index=False)\n",
    "methods_df.to_csv(NEO4J_PATH + \"methods.csv\", index=False)\n",
    "repos_df.to_csv(NEO4J_PATH + \"repos.csv\", index=False)\n",
    "areas_df.to_csv(NEO4J_PATH + \"areas.csv\", index=False)\n",
    "tasks_df.to_csv(NEO4J_PATH + \"tasks.csv\", index=False)\n",
    "datasets_df.to_csv(NEO4J_PATH + \"datasets.csv\", index=False)\n",
    "datasets_tasks_df.to_csv(NEO4J_PATH + \"datasets_tasks.csv\", index=False)\n",
    "papers_tasks_df.to_csv(NEO4J_PATH + \"papers_tasks.csv\", index=False)\n",
    "papers_methods_df.to_csv(NEO4J_PATH + \"papers_methods.csv\", index=False)\n",
    "papers_repos_df.to_csv(NEO4J_PATH + \"papers_repos.csv\", index=False)\n",
    "tasks_areas_df.to_csv(NEO4J_PATH + \"tasks_areas.csv\", index=False)\n",
    "keywords_df.to_csv(NEO4J_PATH + \"keywords.csv\", index=False)\n",
    "papers_title_keywords_df.to_csv(NEO4J_PATH + \"papers_title_keywords.csv\", index=False)\n",
    "papers_abstract_keywords_df.to_csv(NEO4J_PATH + \"papers_abstract_keywords.csv\", index=False)\n",
    "methods_keywords_df.to_csv(NEO4J_PATH + \"methods_keywords.csv\", index=False)\n",
    "datasets_keywords_df.to_csv(NEO4J_PATH + \"datasets_keywords.csv\", index=False)\n",
    "tasks_keywords_df.to_csv(NEO4J_PATH + \"tasks_keywords.csv\", index=False)\n",
    "print(\"Done saving CSV files to neo4j import folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating neo4j admin-import command...\n",
      "\n",
      "=== COMMAND ===\n",
      "\n",
      "bin/neo4j-admin database import full --nodes=import/authors.csv --nodes=import/institutions.csv --nodes=import/fulltexts.csv --nodes=import/papers.csv --nodes=import/methods.csv --nodes=import/repos.csv --nodes=import/areas.csv --nodes=import/tasks.csv --nodes=import/datasets.csv --nodes=import/keywords.csv --relationships=import/authors_papers.csv --relationships=import/authors_institutions.csv --relationships=import/papers_citations.csv --relationships=import/papers_fulltexts.csv --relationships=import/datasets_tasks.csv --relationships=import/papers_tasks.csv --relationships=import/papers_methods.csv --relationships=import/papers_repos.csv --relationships=import/tasks_areas.csv --relationships=import/papers_keywords.csv --relationships=import/methods_keywords.csv --relationships=import/datasets_keywords.csv --relationships=import/tasks_keywords.csv --relationships=import/papers_title_keywords.csv --relationships=import/papers_abstract_keywords.csv --multiline-fields --read-buffer-size=8000000 --skip-bad-relationships --bad-tolerance=100000000 --skip-duplicate-nodes --overwrite-destination neo4j\n",
      "\n",
      "=== END COMMAND ===\n",
      "\n",
      "Writing a txt file called 'neo4j_import_command.txt' with the command in the neo4j folder...\n",
      "Done creating neo4j admin-import command!\n"
     ]
    }
   ],
   "source": [
    "# Create a neo4j admin-import command to import the data into neo4j\n",
    "print(\"Creating neo4j admin-import command...\")\n",
    "print(\"\")\n",
    "# Look into the neo4j folder and list all csv files that have no underscore in their name\n",
    "node_csv_files = [file for file in os.listdir(NEO4J_PATH) if file.endswith(\".csv\") and \"_\" not in file]\n",
    "relationship_csv_files = [file for file in os.listdir(NEO4J_PATH) if file.endswith(\".csv\") and \"_\" in file]\n",
    "node_csv_files = [file for file in node_csv_files if not file.startswith(\"._\")] # Filter out all files that start with ._ (Mac OS hidden files)\n",
    "relationship_csv_files = [file for file in relationship_csv_files if not file.startswith(\"._\")] # Filter out all files that start with ._ (Mac OS hidden files)\n",
    "neo4j_command_start = \"bin/neo4j-admin database import full \"\n",
    "neo4j_command_end = f\"--multiline-fields --read-buffer-size=8000000 --skip-bad-relationships --bad-tolerance=100000000 --skip-duplicate-nodes\"\n",
    "if REPLACE_DB:\n",
    "    neo4j_command_end += \" --overwrite-destination\"\n",
    "    \n",
    "# Add a \"--nodes=import/...\" for each node csv file\n",
    "neo4j_command_nodes = \"\"\n",
    "for node_csv_file in node_csv_files:\n",
    "    neo4j_command_nodes += f\"--nodes=import/{node_csv_file} \"\n",
    "    \n",
    "# Add a \"--relationships=import/...\" for each relationship csv file\n",
    "neo4j_command_relationships = \"\"\n",
    "for relationship_csv_file in relationship_csv_files:\n",
    "    neo4j_command_relationships += f\"--relationships=import/{relationship_csv_file} \"\n",
    "    \n",
    "# Combine the command parts\n",
    "command = neo4j_command_start + neo4j_command_nodes + neo4j_command_relationships + neo4j_command_end + f\" {NEO4J_DB_NAME}\"\n",
    "print(\"=== COMMAND ===\")\n",
    "print(\"\")\n",
    "print(command)\n",
    "print(\"\")\n",
    "print(\"=== END COMMAND ===\")\n",
    "print(\"\")\n",
    "\n",
    "# Write a txt file called \"neo4j_import_command.txt\" with the command in the neo4j folder\n",
    "print(\"Writing a txt file called 'neo4j_import_command.txt' with the command in the neo4j folder...\")\n",
    "with open(NEO4J_PATH + \"neo4j_import_command.txt\", \"w\") as text_file:\n",
    "    text_file.write(command)\n",
    "\n",
    "print(\"Done creating neo4j admin-import command!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the CSVs into Neo4j\n",
    "Now is the time for you to copy the CSV files into the import folder of your neo4j installation (either local or server/docker). Grab the import command from the notebook and run it in the neo4j terminal. The import should not take long.\n",
    "\n",
    "Adhere to the following steps:\n",
    "1. Stop the server\n",
    "2. Copy the CSV files into the import folder of your DBMS\n",
    "3. Run the admin -import tool and use a new database name (e.g. patentsview). You can also choose an existing one but with the flag --overwrite-destination.\n",
    "4. Start the server\n",
    "5. In the console switch to the System database using the pulldown control\n",
    "6. Run the following command at the system prompt: create database aDatabaseName (should be the same name set in #2 above)\n",
    "7. Switch to the database just created using the console pulldown control\n",
    "\n",
    "Hint: You can skip steps 5 to 7 if you are on the desktop version. Simply create a new database in the desktop version and then switch to it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
