{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a negative Dictionary for Keyword Extraction\n",
    "This notebook requires a running instance of a OpenAlex Postgres Database, a neo4j Graph Database as well as core and extended AI dictionaries built. It will create a negative dictionary sampled from the OpenAlex database. The negative dictionary will be used to filter out keywords that are not relevant to the AI domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from sqlalchemy import create_engine, URL, text\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# register tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "import ahocorasick\n",
    "import pickle, re\n",
    "\n",
    "from helper.keyword_helper import get_clean_keywords, neo4j_fetch_data, make_aho_automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHO_LENGTH = 3 # Paper Abstract has to contain AHO_LENGTH or more core keywords to be labeled as ai and deleted from neg sample\n",
    "OALEX_SAMPLE_LIMIT = 500000 # Remember, around 60% will be filtered out...\n",
    "# OALEX_SAMPLE_FRAC = 0.01\n",
    "KEYWORD_FREQ_RANGE = (7,OALEX_SAMPLE_LIMIT) # (min, max) keyword frequency in oalex\n",
    "\n",
    "DICT_PATH = \"data/dictionaries\"\n",
    "\n",
    "url_object = URL.create(\n",
    "    drivername='postgresql+psycopg2',\n",
    "    username='tie',\n",
    "    password='TIE%2023!tuhh',\n",
    "    host='134.28.58.100',\n",
    "    # host='tie-workstation.tail6716.ts.net',\n",
    "    # host='localhost',\n",
    "    port=45432,\n",
    "    database='openalex_db',\n",
    ")\n",
    "engine = create_engine(url_object)\n",
    "engine.connect()\n",
    "\n",
    "# Create a dict of neo4j credentials\n",
    "NEO4J_CREDENTIALS = {\"url\": \"bolt://localhost:37687\", \"user\": \"neo4j\", \"password\": \"neo4jpassword\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 500000 OpenAlex Works\n"
     ]
    }
   ],
   "source": [
    "# Get a Sample of around 0.01% of the data\n",
    "# sql_query = f'''\n",
    "#     SELECT *\n",
    "#     FROM openalex.works\n",
    "#     TABLESAMPLE SYSTEM ({OALEX_SAMPLE_FRAC})\n",
    "#     WHERE abstract_inverted_index IS NOT NULL\n",
    "# '''\n",
    "\n",
    "sql_query = f'''\n",
    "    SELECT *\n",
    "    FROM openalex.works TABLESAMPLE BERNOULLI(10)\n",
    "    WHERE abstract_inverted_index IS NOT NULL\n",
    "    LIMIT {OALEX_SAMPLE_LIMIT}\n",
    "'''\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    openalex = pd.read_sql(sql=text(sql_query), con=conn)\n",
    "    print(f\"Got {len(openalex)} OpenAlex Works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "Done.\n",
      "Got 256414 OpenAlex IDs from PwC Papers.\n"
     ]
    }
   ],
   "source": [
    "# Get all openalex_ids from papers for a negative list\n",
    "query = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.id_openalex IS NOT NULL\n",
    "RETURN p.id_openalex as openalex_id\n",
    "\"\"\"\n",
    "print(\"Fetching data...\")\n",
    "pwc_oalex_ids = neo4j_fetch_data(query, NEO4J_CREDENTIALS)\n",
    "print(\"Done.\")\n",
    "print(f\"Got {len(pwc_oalex_ids)} OpenAlex IDs from PwC Papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of openalex sample abstracts before removing PwC papers: 500000\n",
      "Number of openalex sample abstracts after removing PwC papers: 498905 (99.8% remain)\n"
     ]
    }
   ],
   "source": [
    "# Remove all openalex rows, where the id is in the pwc_oalex_ids df\n",
    "print(f\"Number of openalex sample abstracts before removing PwC papers: {len(openalex)}\")\n",
    "openalex_id_filtered = openalex[~openalex['id'].isin(pwc_oalex_ids['openalex_id'])]\n",
    "print(f\"Number of openalex sample abstracts after removing PwC papers: {len(openalex_id_filtered)} ({round(len(openalex_id_filtered)/len(openalex), 3) * 100}% remain)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after cleaning: 469720 (94.0% remain)\n"
     ]
    }
   ],
   "source": [
    "# Some cleaning\n",
    "openalex_sample = openalex_id_filtered.copy()\n",
    "openalex_sample['abstract_inverted_index'] = openalex_sample['abstract_inverted_index'].apply(lambda x: x['InvertedIndex'])\n",
    "openalex_sample['abstract'] = [\" \".join(list(d.keys())) if d else None for d in openalex_sample['abstract_inverted_index']]\n",
    "openalex_sample.drop(columns=['abstract_inverted_index'], inplace=True)\n",
    "openalex_sample['abstract'] = (openalex_sample['abstract']\n",
    "                                .str.replace(r\"[^a-zA-Z0-9 ]\", \" \", regex=True)\n",
    "                                .str.replace(r\"abstract\", \"\", flags=re.IGNORECASE)\n",
    "                                .str.strip()\n",
    "                                .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                                .astype(str)\n",
    "                                .str.lower()\n",
    "                                )\n",
    "openalex_sample['abstract'] = openalex_sample['abstract'].apply(lambda x: x if len(x.split()) > 10 else None)\n",
    "openalex_sample = openalex_sample[openalex_sample['abstract'].notna()]\n",
    "openalex_sample.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of rows after cleaning: {len(openalex_sample)} ({round((len(openalex_sample))/len(openalex_id_filtered), 2) * 100}% remain)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aho automation...\n",
      "Applying aho automation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e9639ba6484a1e92f7447bcdbc912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the aho automation for the core keywords\n",
    "print(\"Loading aho automation...\")\n",
    "cso_aho_automation = ahocorasick.load('data/dictionaries/core_aho_automation/cso_aho_automation.pkl', pickle.loads)\n",
    "\n",
    "openalex_sample_aho = openalex_sample.copy()\n",
    "# Apply the automation on the abstracts and make a new column with all the results\n",
    "print(\"Applying aho automation...\")\n",
    "openalex_sample_aho['aho_results'] = openalex_sample_aho['abstract'].progress_apply(lambda x: list(cso_aho_automation.iter_long(x)))\n",
    "# Extract only the keywords, not the positions\n",
    "openalex_sample_aho['aho_results'] = openalex_sample_aho['aho_results'].apply(lambda x: [y[1][1] for y in x])\n",
    "# Make a new column aho_length with the length of the results\n",
    "openalex_sample_aho['aho_length'] = openalex_sample_aho['aho_results'].apply(lambda x: len(x))\n",
    "# Sort by aho_length descending\n",
    "openalex_sample_aho.sort_values(by='aho_length', ascending=False, inplace=True)\n",
    "# Reset the index\n",
    "openalex_sample_aho.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found all core ai keywords in abstracts, will now remove all rows with an aho_length of 3 or more.\n",
      "Number of rows after removing all rows with an aho_length of 3 or more: 65613 (14.000000000000002% remain)\n"
     ]
    }
   ],
   "source": [
    "openalex_sample_aho_cut = openalex_sample_aho.copy()\n",
    "# Remove all rows with an aho_length of x or more\n",
    "print(f\"Found all core ai keywords in abstracts, will now remove all rows with an aho_length of {AHO_LENGTH} or more.\")\n",
    "openalex_sample_aho_cut = openalex_sample_aho_cut[openalex_sample_aho_cut['aho_length'] < AHO_LENGTH]\n",
    "print(f\"Number of rows after removing all rows with an aho_length of {AHO_LENGTH} or more: {len(openalex_sample_aho_cut)} ({round(len(openalex_sample_aho_cut)/len(openalex_sample), 3) * 100}% remain)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating keywords for abstract...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538cfd9027a44391832f16bd8eeab6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords for abstract\n",
      "Filtered out keywords with less than 3 characters.\n",
      "Ended up with 382086 keywords.\n"
     ]
    }
   ],
   "source": [
    "# Get the keywords (this will take some time)\n",
    "negative_keywords_df = get_clean_keywords(openalex_sample_aho_cut, [\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 382086 keywords including title and abstract keywords.\n",
      "Got 3041 unique keywords in the list named all_negative_keywords_dedupe (0.8% remain).\n"
     ]
    }
   ],
   "source": [
    "# Make a list with all keywords\n",
    "negative_abstract_keywords = negative_keywords_df.abstract_keywords.tolist()\n",
    "all_negative_keywords = [keyword for keywords in negative_abstract_keywords for keyword, score in keywords]\n",
    "print(f\"Got {len(all_negative_keywords)} keywords including title and abstract keywords.\")\n",
    "\n",
    "# Make a df out of it\n",
    "all_negative_keywords_df = pd.DataFrame(all_negative_keywords, columns=['keyword'])\n",
    "# I want you to convert the all_negative_keywords_df so that only unique keywords remain. But I need the number of occurences for each keyword in the original df. Therefore the unique df has a new column \"frequency\" that contains the number of occurences of the keyword in the original df.\n",
    "all_negative_keywords_df['frequency'] = all_negative_keywords_df['keyword'].map(all_negative_keywords_df['keyword'].value_counts())\n",
    "# Drop duplicates\n",
    "all_negative_keywords_df.drop_duplicates(inplace=True)\n",
    "# Drop all rows where the frequency is not in KEYWORD_FREQ_RANGE\n",
    "all_negative_keywords_df = all_negative_keywords_df[all_negative_keywords_df['frequency'].between(KEYWORD_FREQ_RANGE[0], KEYWORD_FREQ_RANGE[1])]\n",
    "# Reset index\n",
    "all_negative_keywords_df.reset_index(drop=True, inplace=True)\n",
    "# Make a list out of it\n",
    "all_negative_keywords_dedupe = all_negative_keywords_df.keyword.tolist()\n",
    "\n",
    "print(f\"Got {len(all_negative_keywords_dedupe)} unique keywords in the list named all_negative_keywords_dedupe ({round(len(all_negative_keywords_dedupe)/len(all_negative_keywords), 3) * 100}% remain).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading core dictionaries...\n",
      "Done.\n",
      "Loading extended dictionaries...\n",
      "Done.\n",
      "Subtracting all_negative_keywords_dedupe from each core dict...\n",
      "Number of rows in cso_ext after removing all_negative_keywords_dedupe: 1688 out of 1807 (93.4% remain)\n",
      "Number of rows in method_ext after removing all_negative_keywords_dedupe: 1470 out of 1565 (93.89999999999999% remain)\n",
      "Number of rows in task_ext after removing all_negative_keywords_dedupe: 3013 out of 3319 (90.8% remain)\n",
      "Number of rows in dataset_ext after removing all_negative_keywords_dedupe: 6632 out of 7034 (94.3% remain)\n",
      "Done.\n",
      "Subtracting all_negative_keywords_dedupe from each ext dict...\n",
      "Number of rows in cso_ext after removing all_negative_keywords_dedupe: 8997 out of 9182 (98.0% remain)\n",
      "Number of rows in method_ext after removing all_negative_keywords_dedupe: 4421 out of 4548 (97.2% remain)\n",
      "Number of rows in task_ext after removing all_negative_keywords_dedupe: 12819 out of 13013 (98.5% remain)\n",
      "Number of rows in dataset_ext after removing all_negative_keywords_dedupe: 11098 out of 11364 (97.7% remain)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Subtract the all_negative_keywords_dedupe from each ext dict\n",
    "\n",
    "# Make a new folder if it doesn't exist \"data/dictionaries/extended_keywords/\"\n",
    "import os\n",
    "if not os.path.exists('data/dictionaries/core_keywords_neg/'):\n",
    "    os.makedirs('data/dictionaries/core_keywords_neg/')\n",
    "if not os.path.exists('data/dictionaries/extended_keywords_neg/'):\n",
    "    os.makedirs('data/dictionaries/extended_keywords_neg/')\n",
    "    \n",
    "    \n",
    "# Load the core dictionaries\n",
    "print(\"Loading core dictionaries...\")\n",
    "core_keywords = pd.read_csv('data/dictionaries/core_keywords.csv')\n",
    "cso_core = core_keywords[core_keywords['source'] == 'cso']\n",
    "method_core = core_keywords[core_keywords['source'] == 'method']\n",
    "task_core = core_keywords[core_keywords['source'] == 'task']\n",
    "dataset_core = core_keywords[core_keywords['source'] == 'dataset']\n",
    "print(\"Done.\")\n",
    "\n",
    "# Load the extended dictionaries\n",
    "print(\"Loading extended dictionaries...\")\n",
    "extended_keywords = pd.read_csv('data/dictionaries/extended_keywords.csv')\n",
    "cso_ext = extended_keywords[extended_keywords['source'] == 'cso']\n",
    "method_ext = extended_keywords[extended_keywords['source'] == 'method']\n",
    "task_ext = extended_keywords[extended_keywords['source'] == 'task']\n",
    "dataset_ext = extended_keywords[extended_keywords['source'] == 'dataset']\n",
    "print(\"Done.\")\n",
    "\n",
    "# Subtract the all_negative_keywords_dedupe from each ext dict\n",
    "print(\"Subtracting all_negative_keywords_dedupe from each core dict...\")\n",
    "\n",
    "all_keywords_core = pd.DataFrame(columns=['keyword', 'source'])\n",
    "\n",
    "for ext, name in [(cso_core, 'cso'), (method_core, 'method'), (task_core, 'task'), (dataset_core, 'dataset')]:\n",
    "    initial_length_ext = len(ext)\n",
    "    removed_keywords = ext[ext['keyword'].isin(all_negative_keywords_dedupe)]\n",
    "    ext = ext[~ext['keyword'].isin(all_negative_keywords_dedupe)]\n",
    "    ext.dropna(inplace=True)\n",
    "    ext.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Number of rows in {name}_ext after removing all_negative_keywords_dedupe: {len(ext)} out of {initial_length_ext} ({round(len(ext)/initial_length_ext, 3) * 100}% remain)\")\n",
    "    # Save the removed keywords to a CSV file\n",
    "    removed_keywords.to_csv(f'data/dictionaries/core_keywords_neg/removed_keywords_core_{name}.csv', index=False)\n",
    "    # Make a new column \"source\" with the name of the dictionary\n",
    "    ext['source'] = name\n",
    "    # Concatenate this df with all_keywords_core\n",
    "    all_keywords_core = pd.concat([all_keywords_core, ext], ignore_index=True)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Subtract the all_negative_keywords_dedupe from each ext dict\n",
    "print(\"Subtracting all_negative_keywords_dedupe from each ext dict...\")\n",
    "\n",
    "all_keywords_ext = pd.DataFrame(columns=['keyword', 'source'])\n",
    "\n",
    "for ext, name in [(cso_ext, 'cso'), (method_ext, 'method'), (task_ext, 'task'), (dataset_ext, 'dataset')]:\n",
    "    initial_length_ext = len(ext)\n",
    "    removed_keywords = ext[ext['keyword'].isin(all_negative_keywords_dedupe)]\n",
    "    ext = ext[~ext['keyword'].isin(all_negative_keywords_dedupe)]\n",
    "    ext.dropna(inplace=True)\n",
    "    ext.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Number of rows in {name}_ext after removing all_negative_keywords_dedupe: {len(ext)} out of {initial_length_ext} ({round(len(ext)/initial_length_ext, 3) * 100}% remain)\")\n",
    "    # Save the removed keywords to a CSV file\n",
    "    removed_keywords.to_csv(f'data/dictionaries/extended_keywords_neg/removed_keywords_extended_{name}.csv', index=False)\n",
    "    # Make a new column \"source\" with the name of the dictionary\n",
    "    ext['source'] = name\n",
    "    # Concatenate this df with all_keywords_ext\n",
    "    all_keywords_ext = pd.concat([all_keywords_ext, ext], ignore_index=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making aho automations for the core keywords...\n",
      "Making aho automations for the extended keywords...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Make a new folder in the DICT_PATH for the ahocorasick dumps\n",
    "if not os.path.exists(DICT_PATH + \"/core_neg_aho_automation\"):\n",
    "    os.mkdir(DICT_PATH + \"/core_neg_aho_automation\")\n",
    "if not os.path.exists(DICT_PATH + \"/extended_neg_aho_automation\"):\n",
    "    os.mkdir(DICT_PATH + \"/extended_neg_aho_automation\")\n",
    "    \n",
    "# Make aho automations for the core keywords\n",
    "print(\"Making aho automations for the core keywords...\")\n",
    "core_neg_keywords_cso_automation = make_aho_automation(all_keywords_core[all_keywords_core['source'] == 'cso'].keyword.tolist())\n",
    "core_neg_keywords_cso_automation.save(DICT_PATH + \"/core_neg_aho_automation/cso_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "core_neg_keywords_method_automation = make_aho_automation(all_keywords_core[all_keywords_core['source'] == 'method'].keyword.tolist())\n",
    "core_neg_keywords_method_automation.save(DICT_PATH + \"/core_neg_aho_automation/method_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "core_neg_keywords_task_automation = make_aho_automation(all_keywords_core[all_keywords_core['source'] == 'task'].keyword.tolist())\n",
    "core_neg_keywords_task_automation.save(DICT_PATH + \"/core_neg_aho_automation/task_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "core_neg_keywords_dataset_automation = make_aho_automation(all_keywords_core[all_keywords_core['source'] == 'dataset'].keyword.tolist())\n",
    "core_neg_keywords_dataset_automation.save(DICT_PATH + \"/core_neg_aho_automation/dataset_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "# Make aho automations for the extended keywords\n",
    "print(\"Making aho automations for the extended keywords...\")    \n",
    "extended_neg_keywords_cso_automation = make_aho_automation(all_keywords_ext[all_keywords_ext['source'] == 'cso'].keyword.tolist())\n",
    "extended_neg_keywords_cso_automation.save(DICT_PATH + \"/extended_neg_aho_automation/cso_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_neg_keywords_method_automation = make_aho_automation(all_keywords_ext[all_keywords_ext['source'] == 'method'].keyword.tolist())\n",
    "extended_neg_keywords_method_automation.save(DICT_PATH + \"/extended_neg_aho_automation/method_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_neg_keywords_task_automation = make_aho_automation(all_keywords_ext[all_keywords_ext['source'] == 'task'].keyword.tolist())\n",
    "extended_neg_keywords_task_automation.save(DICT_PATH + \"/extended_neg_aho_automation/task_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_neg_keywords_dataset_automation = make_aho_automation(all_keywords_ext[all_keywords_ext['source'] == 'dataset'].keyword.tolist())\n",
    "extended_neg_keywords_dataset_automation.save(DICT_PATH + \"/extended_neg_aho_automation/dataset_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving core dictionary...\n",
      "Saving extended dictionary...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Save the extended dictionaries\n",
    "print(\"Saving core dictionary...\")\n",
    "all_keywords_core.to_csv(DICT_PATH + '/core_keywords_neg.csv', index=False)\n",
    "print(\"Saving extended dictionary...\")\n",
    "all_keywords_ext.to_csv(DICT_PATH + '/extended_keywords_neg.csv', index=False)\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
