{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Extended AI Dictionary\n",
    "This notebook requires a running instance of the neo4j Graph Database with all the data from the steps before loaded and a built core dictionary. It will extend the core dictionary with the data from the graph database and save it as a new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle, os\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# register tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "from helper.keyword_helper import process_keywords_for_papers, make_aho_automation, neo4j_fetch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 20\n",
    "\n",
    "KEYWORD_FREQ_RANGE = (4,5000)\n",
    "COS_THRESHOLD = 0.95\n",
    "\n",
    "DICT_PATH = \"data/dictionaries\"\n",
    "\n",
    "# Create a dict of neo4j credentials\n",
    "NEO4J_CREDENTIALS = {\"url\": \"bolt://localhost:37687\", \"user\": \"neo4j\", \"password\": \"neo4jpassword\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "Done.\n",
      "Number of keywords: 4429807\n"
     ]
    }
   ],
   "source": [
    "# Get all keywords related to papers - takes around 2 minutes\n",
    "query = \"\"\"\n",
    "MATCH (p:Paper)-[r]->(k:Keyword)\n",
    "WITH k.keyword AS keyword, p.title AS paper_title, p.id AS paper_id\n",
    "RETURN keyword, paper_title, paper_id\n",
    "\"\"\"\n",
    "print(\"Fetching data...\")\n",
    "paper_keywords = neo4j_fetch_data(query, NEO4J_CREDENTIALS)\n",
    "print(\"Done.\")\n",
    "print(f\"Number of keywords: {len(paper_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurences of the keyword 'model': 144068\n",
      "Got 887067 keywords after deduplication.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>frequency</th>\n",
       "      <th>frequency_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model</td>\n",
       "      <td>76353</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>65387</td>\n",
       "      <td>0.856378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>method</td>\n",
       "      <td>44093</td>\n",
       "      <td>0.577489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>task</td>\n",
       "      <td>41994</td>\n",
       "      <td>0.549998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paper</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.392912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>image</td>\n",
       "      <td>29576</td>\n",
       "      <td>0.387359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>approach</td>\n",
       "      <td>25300</td>\n",
       "      <td>0.331356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deep</td>\n",
       "      <td>23491</td>\n",
       "      <td>0.307663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>work</td>\n",
       "      <td>22360</td>\n",
       "      <td>0.292850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>20691</td>\n",
       "      <td>0.270991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>system</td>\n",
       "      <td>20474</td>\n",
       "      <td>0.268149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>network</td>\n",
       "      <td>20401</td>\n",
       "      <td>0.267193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>learning</td>\n",
       "      <td>20209</td>\n",
       "      <td>0.264679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>problem</td>\n",
       "      <td>18454</td>\n",
       "      <td>0.241693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>time</td>\n",
       "      <td>13965</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>proposed</td>\n",
       "      <td>13012</td>\n",
       "      <td>0.170419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>neural</td>\n",
       "      <td>12484</td>\n",
       "      <td>0.163504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>framework</td>\n",
       "      <td>12453</td>\n",
       "      <td>0.163098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>feature</td>\n",
       "      <td>11933</td>\n",
       "      <td>0.156287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>text</td>\n",
       "      <td>11408</td>\n",
       "      <td>0.149411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>training</td>\n",
       "      <td>11403</td>\n",
       "      <td>0.149346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>human</td>\n",
       "      <td>11153</td>\n",
       "      <td>0.146072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>object</td>\n",
       "      <td>10760</td>\n",
       "      <td>0.140924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>input</td>\n",
       "      <td>10086</td>\n",
       "      <td>0.132097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>9955</td>\n",
       "      <td>0.130381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>graph</td>\n",
       "      <td>9423</td>\n",
       "      <td>0.123414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Deep</td>\n",
       "      <td>9406</td>\n",
       "      <td>0.123191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>high</td>\n",
       "      <td>9179</td>\n",
       "      <td>0.120218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>study</td>\n",
       "      <td>9041</td>\n",
       "      <td>0.118411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>user</td>\n",
       "      <td>8535</td>\n",
       "      <td>0.111783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>result</td>\n",
       "      <td>8093</td>\n",
       "      <td>0.105995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>video</td>\n",
       "      <td>7878</td>\n",
       "      <td>0.103179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>performance</td>\n",
       "      <td>7849</td>\n",
       "      <td>0.102799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>large</td>\n",
       "      <td>7658</td>\n",
       "      <td>0.100297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>based</td>\n",
       "      <td>7400</td>\n",
       "      <td>0.096918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>deep neural network</td>\n",
       "      <td>7213</td>\n",
       "      <td>0.094469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>analysis</td>\n",
       "      <td>7012</td>\n",
       "      <td>0.091837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>neural network</td>\n",
       "      <td>6976</td>\n",
       "      <td>0.091365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>language</td>\n",
       "      <td>6793</td>\n",
       "      <td>0.088968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>field</td>\n",
       "      <td>6733</td>\n",
       "      <td>0.088183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>number</td>\n",
       "      <td>6721</td>\n",
       "      <td>0.088025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>6703</td>\n",
       "      <td>0.087790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>domain</td>\n",
       "      <td>6524</td>\n",
       "      <td>0.085445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>loss</td>\n",
       "      <td>6524</td>\n",
       "      <td>0.085445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>space</td>\n",
       "      <td>6431</td>\n",
       "      <td>0.084227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>function</td>\n",
       "      <td>6181</td>\n",
       "      <td>0.080953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>word</td>\n",
       "      <td>6158</td>\n",
       "      <td>0.080652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>agent</td>\n",
       "      <td>6093</td>\n",
       "      <td>0.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>technique</td>\n",
       "      <td>5911</td>\n",
       "      <td>0.077417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>datasets</td>\n",
       "      <td>5380</td>\n",
       "      <td>0.070462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword  frequency  frequency_normalized\n",
       "0                 model      76353              1.000000\n",
       "1                  data      65387              0.856378\n",
       "2                method      44093              0.577489\n",
       "3                  task      41994              0.549998\n",
       "4                 paper      30000              0.392912\n",
       "5                 image      29576              0.387359\n",
       "6              approach      25300              0.331356\n",
       "7                  deep      23491              0.307663\n",
       "8                  work      22360              0.292850\n",
       "9             algorithm      20691              0.270991\n",
       "10               system      20474              0.268149\n",
       "11              network      20401              0.267193\n",
       "12             learning      20209              0.264679\n",
       "13              problem      18454              0.241693\n",
       "14                 time      13965              0.182900\n",
       "15             proposed      13012              0.170419\n",
       "16               neural      12484              0.163504\n",
       "17            framework      12453              0.163098\n",
       "18              feature      11933              0.156287\n",
       "19                 text      11408              0.149411\n",
       "20             training      11403              0.149346\n",
       "21                human      11153              0.146072\n",
       "22               object      10760              0.140924\n",
       "23                input      10086              0.132097\n",
       "24             accuracy       9955              0.130381\n",
       "25                graph       9423              0.123414\n",
       "26                 Deep       9406              0.123191\n",
       "27                 high       9179              0.120218\n",
       "28                study       9041              0.118411\n",
       "29                 user       8535              0.111783\n",
       "30               result       8093              0.105995\n",
       "31                video       7878              0.103179\n",
       "32          performance       7849              0.102799\n",
       "33                large       7658              0.100297\n",
       "34                based       7400              0.096918\n",
       "35  deep neural network       7213              0.094469\n",
       "36             analysis       7012              0.091837\n",
       "37       neural network       6976              0.091365\n",
       "38             language       6793              0.088968\n",
       "39                field       6733              0.088183\n",
       "40               number       6721              0.088025\n",
       "41            knowledge       6703              0.087790\n",
       "42               domain       6524              0.085445\n",
       "43                 loss       6524              0.085445\n",
       "44                space       6431              0.084227\n",
       "45             function       6181              0.080953\n",
       "46                 word       6158              0.080652\n",
       "47                agent       6093              0.079800\n",
       "48            technique       5911              0.077417\n",
       "49             datasets       5380              0.070462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dedupe_keywords_f = (paper_keywords\n",
    "                     .value_counts(subset=['keyword'])\n",
    "                     .reset_index(name='frequency')\n",
    "                     .sort_values(by='frequency', ascending=False)\n",
    "                     .assign(frequency_normalized=lambda df: df['frequency'] / df['frequency'].max()))\n",
    "\n",
    "print(f\"Got {len(dedupe_keywords_f)} keywords after deduplication.\")\n",
    "display(dedupe_keywords_f.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model...\n",
      "Done.\n",
      "Embedding all keywords...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f0a8d8fb1249918c03f6bb12b5a2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading Sentence Transformer model...\")\n",
    "model_scincl = SentenceTransformer('malteos/SciNCL')\n",
    "print(\"Done.\")\n",
    "print(\"Embedding all keywords...\")\n",
    "dedupe_keywords_f['embedding'] = model_scincl.encode(dedupe_keywords_f['keyword'].tolist(), show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the core keywords with their embeddings into a df\n",
    "core_keywords = pd.read_csv('data/dictionaries/core_keywords.csv')\n",
    "core_keywords['embedding'] = core_keywords['embedding'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar keywords...\n",
      "\n",
      "Got 9182 unique extended cso keywords after processing core keywords (10.98%)\n",
      "Got 4548 unique extended method keywords after processing core keywords (5.44%)\n",
      "Got 13013 unique extended task keywords after processing core keywords (15.56%)\n",
      "Got 11364 unique extended dataset keywords after processing core keywords (13.59%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating similarity - this step will take a while (around 1h)\n",
    "\n",
    "core_keywords_cso = core_keywords[core_keywords['source'] == 'cso']\n",
    "core_keywords_method = core_keywords[core_keywords['source'] == 'method']\n",
    "core_keywords_task = core_keywords[core_keywords['source'] == 'task']\n",
    "core_keywords_dataset = core_keywords[core_keywords['source'] == 'dataset']\n",
    "\n",
    "# Assuming core_keywords_sample and dedupe_keywords_f are already defined\n",
    "\n",
    "def get_keywords_above_threshold(core_embedding, extended_embeddings, cos_threshold=0.1):\n",
    "    similarities = cosine_similarity([core_embedding], extended_embeddings)[0]\n",
    "    return np.where(similarities > cos_threshold)[0]\n",
    "\n",
    "def batch_process_embeddings(core_embeddings, extended_embeddings, cos_threshold):\n",
    "    with ThreadPoolExecutor(max_workers=NUM_CORES) as executor:\n",
    "        results = list(executor.map(lambda embedding: get_keywords_above_threshold(embedding, extended_embeddings, cos_threshold), core_embeddings))\n",
    "    return results\n",
    "\n",
    "def process_keywords(df, dedupe_keywords, cos_threshold, source):\n",
    "    df['keywords_above_threshold'] = batch_process_embeddings(df['embedding'].tolist(), np.array(dedupe_keywords['embedding'].tolist()), cos_threshold)\n",
    "    df = df.drop(columns=['embedding'])\n",
    "    df['keywords_above_threshold'] = df['keywords_above_threshold'].apply(lambda indices: dedupe_keywords.iloc[indices]['keyword'].tolist())\n",
    "    df = df[['keyword', 'source', 'keywords_above_threshold']]\n",
    "    df['keywords_above_threshold'] = df['keywords_above_threshold'].apply(lambda keywords: list(set(keywords) - set(df['keyword'].tolist())))\n",
    "    df_all = pd.DataFrame({keyword for keywords in df['keywords_above_threshold'] for keyword in keywords}, columns=['keyword'])\n",
    "    df_all['source'] = source\n",
    "    return df_all\n",
    "\n",
    "print(\"Finding similar keywords...\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Processing core keywords...\")\n",
    "extended_keywords_cso = process_keywords(core_keywords_cso, dedupe_keywords_f, COS_THRESHOLD, \"cso\")\n",
    "print(f\"Got {len(extended_keywords_cso)} unique extended cso keywords after processing core keywords ({len(extended_keywords_cso) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "\n",
    "print(\"Processing method keywords...\")\n",
    "extended_keywords_method = process_keywords(core_keywords_method, dedupe_keywords_f, COS_THRESHOLD, \"method\")\n",
    "print(f\"Got {len(extended_keywords_method)} unique extended method keywords after processing core keywords ({len(extended_keywords_method) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "\n",
    "print(\"Processing task keywords...\")\n",
    "extended_keywords_task = process_keywords(core_keywords_task, dedupe_keywords_f, COS_THRESHOLD, \"task\")\n",
    "print(f\"Got {len(extended_keywords_task)} unique extended task keywords after processing core keywords ({len(extended_keywords_task) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "\n",
    "print(\"Processing dataset keywords...\")\n",
    "extended_keywords_dataset = process_keywords(core_keywords_dataset, dedupe_keywords_f, COS_THRESHOLD, \"dataset\")\n",
    "print(f\"Got {len(extended_keywords_dataset)} unique extended dataset keywords after processing core keywords ({len(extended_keywords_dataset) / len(dedupe_keywords_f) * 100:.2f}%)\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Done.\")\n",
    "\n",
    "extended_keywords = pd.concat([extended_keywords_cso, extended_keywords_method, extended_keywords_task, extended_keywords_dataset])\n",
    "extended_keywords = extended_keywords.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new folder in the DICT_PATH for the ahocorasick dumps\n",
    "if not os.path.exists(DICT_PATH + \"/extended_aho_automation\"):\n",
    "    os.mkdir(DICT_PATH + \"/extended_aho_automation\")\n",
    "    \n",
    "extended_keywords_cso_automation = make_aho_automation(extended_keywords_cso['keyword'].tolist())\n",
    "extended_keywords_cso_automation.save(f\"{DICT_PATH}/extended_aho_automation/cso_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_keywords_method_automation = make_aho_automation(extended_keywords_method['keyword'].tolist())\n",
    "extended_keywords_method_automation.save(f\"{DICT_PATH}/extended_aho_automation/method_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_keywords_task_automation = make_aho_automation(extended_keywords_task['keyword'].tolist())\n",
    "extended_keywords_task_automation.save(f\"{DICT_PATH}/extended_aho_automation/task_aho_automation.pkl\", pickle.dumps)\n",
    "\n",
    "extended_keywords_dataset_automation = make_aho_automation(extended_keywords_dataset['keyword'].tolist())\n",
    "extended_keywords_dataset_automation.save(f\"{DICT_PATH}/extended_aho_automation/dataset_aho_automation.pkl\", pickle.dumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding all extended keywords...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c048b827cd64424865ff3fe3e6fb04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Calculate embeddings for all extended keywords\n",
    "print(\"Embedding all extended keywords...\")\n",
    "extended_keywords['embedding'] = model_scincl.encode(extended_keywords['keyword'].tolist(), show_progress_bar=True).tolist()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the extended keywords to a csv\n",
    "extended_keywords.to_csv('data/dictionaries/extended_keywords.csv', index=False)\n",
    "print(\"Done saving.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
